{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual RAG Pipeline: Mechanisms First\n",
    "\n",
    "This notebook builds a Retrieval-Augmented Generation (RAG) pipeline from scratch.\n",
    "You'll see every step explicitly before we move to frameworks like LangChain.\n",
    "\n",
    "**Works on:** Google Colab, Local Jupyter (Mac/Windows/Linux)\n",
    "\n",
    "**Pipeline Overview:**\n",
    "```\n",
    "Documents â†’ Chunking â†’ Embedding â†’ Index (FAISS)\n",
    "                                        â†“\n",
    "User Query â†’ Embed Query â†’ Similarity Search â†’ Top-K Chunks\n",
    "                                                    â†“\n",
    "                                        Prompt Assembly â†’ LLM â†’ Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages and detect our compute environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# On Colab, these install quickly. Locally, you may already have them.\n",
    "!pip install -q torch transformers sentence-transformers faiss-cpu pymupdf accelerate ipyfilechooser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT AND DEVICE DETECTION\n",
    "# =============================================================================\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Enable MPS fallback for any PyTorch operations not yet implemented on Metal\n",
    "# This MUST be set before importing torch\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "def detect_environment() -> str:\n",
    "    \"\"\"Detect if we're running on Colab or locally.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except ImportError:\n",
    "        return 'local'\n",
    "\n",
    "def get_device() -> Tuple[str, torch.dtype]:\n",
    "    \"\"\"\n",
    "    Detect the best available compute device.\n",
    "    \n",
    "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (device_string, recommended_dtype)\n",
    "        \n",
    "    Notes:\n",
    "        - CUDA: Use float16 for memory efficiency (Tensor Cores optimize this)\n",
    "        - MPS: Use float32 - Apple Silicon doesn't have the same float16 \n",
    "               optimizations as NVIDIA, and float32 is often faster\n",
    "        - CPU: Use float32 (float16 not well supported on CPU)\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        dtype = torch.float16\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"âœ“ Using CUDA GPU: {device_name} ({memory_gb:.1f} GB)\")\n",
    "        \n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = 'mps'\n",
    "        dtype = torch.float32  # float32 is often faster on Apple Silicon!\n",
    "        print(\"âœ“ Using Apple Silicon GPU (MPS)\")\n",
    "        print(\"  Note: Using float32 (faster than float16 on Apple Silicon)\")\n",
    "        \n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        dtype = torch.float32\n",
    "        print(\"âš  Using CPU (no GPU detected)\")\n",
    "        print(\"  Tip: For faster processing, use a machine with a GPU\")\n",
    "    \n",
    "    return device, dtype\n",
    "\n",
    "# Detect environment and device\n",
    "ENVIRONMENT = detect_environment()\n",
    "DEVICE, DTYPE = get_device()\n",
    "\n",
    "print(f\"\\nEnvironment: {ENVIRONMENT.upper()}\")\n",
    "print(f\"Device: {DEVICE}, Dtype: {DTYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Your Documents\n",
    "\n",
    "**Cell 1:** Configure your document source and select/upload files\n",
    "- **Local Jupyter**: Use the folder picker, then run Cell 2\n",
    "- **Colab + Upload**: Files upload immediately (blocking), then run Cell 2\n",
    "- **Colab + Drive**: Set `USE_GOOGLE_DRIVE = True`, mounts Drive and shows picker, then run Cell 2\n",
    "\n",
    "**Cell 2:** Confirms selection and lists documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: SELECT DOCUMENT SOURCE\n",
    "# =============================================================================\n",
    "# This cell either:\n",
    "#   - Shows a folder picker (Local or Colab+Drive) - NON-BLOCKING\n",
    "#   - Shows an upload dialog (Colab+Upload) - BLOCKING\n",
    "#\n",
    "# If a folder picker is shown, SELECT YOUR FOLDER BEFORE running Cell 2.\n",
    "# The picker widget is non-blocking, so the code continues before you select.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------- COLAB USERS: CONFIGURE HERE -------------\n",
    "USE_GOOGLE_DRIVE = False  # Set to True to use Google Drive instead of uploading\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Default folder (used for Colab uploads and as fallback for local)\n",
    "DOC_FOLDER = 'documents'\n",
    "folder_chooser = None  # Will hold the picker widget if used\n",
    "\n",
    "if ENVIRONMENT == 'colab':\n",
    "    if USE_GOOGLE_DRIVE:\n",
    "        # ----- COLAB + GOOGLE DRIVE -----\n",
    "        # Mount Drive first, then show folder picker\n",
    "        from google.colab import drive\n",
    "        print(\"Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"âœ“ Google Drive mounted\\n\")\n",
    "        \n",
    "        # Now show folder picker for the Drive\n",
    "        try:\n",
    "            from ipyfilechooser import FileChooser\n",
    "            \n",
    "            folder_chooser = FileChooser(\n",
    "                path='/content/drive/MyDrive',\n",
    "                title='Select your documents folder in Google Drive',\n",
    "                show_only_dirs=True,\n",
    "                select_default=True\n",
    "            )\n",
    "            print(\"ðŸ“ Select your documents folder below, then run Cell 2:\")\n",
    "            print(\"   (The picker is non-blocking - select BEFORE running the next cell)\")\n",
    "            display(folder_chooser)\n",
    "            \n",
    "        except ImportError:\n",
    "            # Fallback: manual path entry\n",
    "            print(\"Folder picker not available.\")\n",
    "            print(\"Edit DOC_FOLDER below with your Google Drive path, then run Cell 2:\")\n",
    "            DOC_FOLDER = '/content/drive/MyDrive/your_documents_folder'  # â† Edit this!\n",
    "            print(f\"  DOC_FOLDER = '{DOC_FOLDER}'\")\n",
    "    else:\n",
    "        # ----- COLAB + UPLOAD -----\n",
    "        # Upload dialog blocks until complete, so DOC_FOLDER is ready when done\n",
    "        from google.colab import files\n",
    "        os.makedirs(DOC_FOLDER, exist_ok=True)\n",
    "        \n",
    "        print(\"Upload your documents (PDF, TXT, or MD):\")\n",
    "        print(\"(This dialog blocks until upload is complete)\\n\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        for filename in uploaded.keys():\n",
    "            os.rename(filename, f'{DOC_FOLDER}/{filename}')\n",
    "            print(f\"  âœ“ Saved: {DOC_FOLDER}/{filename}\")\n",
    "        \n",
    "        print(f\"\\nâœ“ Upload complete. Run Cell 2 to continue.\")\n",
    "\n",
    "else:\n",
    "    # ----- LOCAL JUPYTER -----\n",
    "    # Show folder picker\n",
    "    print(\"Running locally\\n\")\n",
    "    \n",
    "    try:\n",
    "        from ipyfilechooser import FileChooser\n",
    "        \n",
    "        folder_chooser = FileChooser(\n",
    "            path=str(Path.home()),\n",
    "            title='Select your documents folder',\n",
    "            show_only_dirs=True,\n",
    "            select_default=True\n",
    "        )\n",
    "        print(\"ðŸ“ Select your documents folder below, then run Cell 2:\")\n",
    "        print(\"   (The picker is non-blocking - select BEFORE running the next cell)\")\n",
    "        display(folder_chooser)\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback: manual path entry\n",
    "        print(\"Folder picker not available (ipyfilechooser not installed).\")\n",
    "        print(f\"\\nUsing default folder: {Path(DOC_FOLDER).absolute()}\")\n",
    "        print(\"\\nTo use a different folder, edit DOC_FOLDER in this cell:\")\n",
    "        print(\"  DOC_FOLDER = '/path/to/your/documents'\")\n",
    "        os.makedirs(DOC_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: CONFIRM SELECTION AND LIST DOCUMENTS\n",
    "# =============================================================================\n",
    "# If you used a folder picker above, make sure you selected a folder\n",
    "# BEFORE running this cell. The picker is non-blocking.\n",
    "# =============================================================================\n",
    "\n",
    "# Read selection from folder picker (if one was used)\n",
    "if folder_chooser is not None and folder_chooser.selected_path:\n",
    "    DOC_FOLDER = folder_chooser.selected_path\n",
    "    print(f\"âœ“ Using selected folder: {DOC_FOLDER}\")\n",
    "elif folder_chooser is not None:\n",
    "    print(\"âš  No folder selected in picker!\")\n",
    "    print(\"  Please go back to Cell 1, select a folder, then run this cell again.\")\n",
    "else:\n",
    "    # No picker used (upload or manual path)\n",
    "    print(f\"âœ“ Using folder: {DOC_FOLDER}\")\n",
    "\n",
    "# ------------- List documents in folder -------------\n",
    "print()\n",
    "print(f\"Documents in '{DOC_FOLDER}':\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "doc_path = Path(DOC_FOLDER)\n",
    "if doc_path.exists():\n",
    "    files_found = [f for f in doc_path.glob('*') if f.is_file()]\n",
    "    supported = [f for f in files_found if f.suffix.lower() in ['.pdf', '.txt', '.md', '.text']]\n",
    "    \n",
    "    if supported:\n",
    "        for f in supported:\n",
    "            size_kb = f.stat().st_size / 1024\n",
    "            print(f\"  âœ“ {f.name} ({size_kb:.1f} KB)\")\n",
    "        print(f\"\\nTotal: {len(supported)} supported file(s)\")\n",
    "    else:\n",
    "        print(\"  (no PDF or TXT files found)\")\n",
    "        if files_found:\n",
    "            print(f\"  Found {len(files_found)} other file(s) - only .pdf, .txt, .md supported\")\n",
    "else:\n",
    "    print(f\"  âš  Folder not found: {DOC_FOLDER}\")\n",
    "    print(\"  Please go back to Cell 1 and select a valid folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 1: Document Loading\n",
    "\n",
    "We need to extract text from our documents. For PDFs with embedded text,\n",
    "PyMuPDF (fitz) reads the text layer directly - no OCR needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from typing import List, Tuple\n",
    "\n",
    "def load_text_file(filepath: str) -> str:\n",
    "    \"\"\"Load a plain text file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def load_pdf_file(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF with embedded text.\n",
    "    \n",
    "    PyMuPDF reads the text layer directly.\n",
    "    For scanned PDFs without embedded text, you'd need OCR.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(filepath)\n",
    "    text_parts = []\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        if text.strip():\n",
    "            # Add page marker for debugging/citation\n",
    "            text_parts.append(f\"\\n[Page {page_num + 1}]\\n{text}\")\n",
    "    \n",
    "    doc.close()\n",
    "    return \"\\n\".join(text_parts)\n",
    "\n",
    "\n",
    "def load_documents(doc_folder: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Load all documents from a folder. Returns list of (filename, content).\"\"\"\n",
    "    documents = []\n",
    "    folder = Path(doc_folder)\n",
    "    \n",
    "    for filepath in folder.rglob(\"*\"):\n",
    "        if filepath.is_file():\n",
    "            try:\n",
    "                if filepath.suffix.lower() == '.pdf':\n",
    "                    content = load_pdf_file(str(filepath))\n",
    "                elif filepath.suffix.lower() in ['.txt', '.md', '.text']:\n",
    "                    content = load_text_file(str(filepath))\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if content.strip():\n",
    "                    documents.append((filepath.name, content))\n",
    "                    print(f\"âœ“ Loaded: {filepath.name} ({len(content):,} chars)\")\n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Error loading {filepath}: {e}\")\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your documents\n",
    "documents = load_documents(DOC_FOLDER)\n",
    "print(f\"\\nLoaded {len(documents)} documents\")\n",
    "\n",
    "if len(documents) == 0:\n",
    "    print(\"\\nâš  No documents loaded! Please add PDF or TXT files to the documents folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a document to verify loading worked\n",
    "if documents:\n",
    "    filename, content = documents[0]\n",
    "    print(f\"First document: {filename}\")\n",
    "    print(f\"Total length: {len(content):,} characters\")\n",
    "    print(f\"\\nFirst 1000 characters:\\n{'-'*40}\")\n",
    "    print(content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 2: Chunking\n",
    "\n",
    "Documents need to be split into pieces small enough to be relevant but large enough to carry meaning.\n",
    "\n",
    "**Why overlap?** If a key sentence sits right at a chunk boundary, splitting without overlap might cut it in half. Overlap ensures that information near boundaries appears intact in at least one chunk.\n",
    "\n",
    "**Experiment:** Try different chunk sizes (256, 512, 1024) and see how it affects retrieval!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"A chunk of text with metadata for tracing back to source.\"\"\"\n",
    "    text: str\n",
    "    source_file: str\n",
    "    chunk_index: int\n",
    "    start_char: int\n",
    "    end_char: int\n",
    "\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    source_file: str,\n",
    "    chunk_size: int = 512,\n",
    "    chunk_overlap: int = 128\n",
    ") -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \n",
    "    We try to break at sentence or paragraph boundaries\n",
    "    to avoid cutting mid-thought.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    chunk_index = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        # Try to break at a good boundary\n",
    "        if end < len(text):\n",
    "            # Look for paragraph break first\n",
    "            para_break = text.rfind('\\n\\n', start + chunk_size // 2, end)\n",
    "            if para_break != -1:\n",
    "                end = para_break + 2\n",
    "            else:\n",
    "                # Look for sentence break\n",
    "                sentence_break = text.rfind('. ', start + chunk_size // 2, end)\n",
    "                if sentence_break != -1:\n",
    "                    end = sentence_break + 2\n",
    "        \n",
    "        chunk_text_str = text[start:end].strip()\n",
    "        \n",
    "        if chunk_text_str:\n",
    "            chunks.append(Chunk(\n",
    "                text=chunk_text_str,\n",
    "                source_file=source_file,\n",
    "                chunk_index=chunk_index,\n",
    "                start_char=start,\n",
    "                end_char=end\n",
    "            ))\n",
    "            chunk_index += 1\n",
    "        \n",
    "        # Move forward, accounting for overlap\n",
    "        start = end - chunk_overlap\n",
    "        if chunks and start <= chunks[-1].start_char:\n",
    "            start = end  # Safety: ensure progress\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXPERIMENT: Try different chunk sizes!\n",
    "# ============================================\n",
    "CHUNK_SIZE = 512      # Try: 256, 512, 1024\n",
    "CHUNK_OVERLAP = 128   # Try: 64, 128, 256\n",
    "\n",
    "# Chunk all documents\n",
    "all_chunks = []\n",
    "for filename, content in documents:\n",
    "    doc_chunks = chunk_text(content, filename, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "    all_chunks.extend(doc_chunks)\n",
    "    print(f\"{filename}: {len(doc_chunks)} chunks\")\n",
    "\n",
    "print(f\"\\nTotal: {len(all_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect some chunks\n",
    "if all_chunks:\n",
    "    print(\"Sample chunks:\")\n",
    "    indices_to_show = [0, len(all_chunks)//2, -1] if len(all_chunks) > 2 else range(len(all_chunks))\n",
    "    for i in indices_to_show:\n",
    "        chunk = all_chunks[i]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Chunk {chunk.chunk_index} from {chunk.source_file}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(chunk.text[:300] + \"...\" if len(chunk.text) > 300 else chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 3: Embedding\n",
    "\n",
    "Embeddings map text to dense vectors where **semantic similarity = geometric proximity**.\n",
    "\n",
    "A sentence about \"cardiac arrest\" and one about \"heart attack\" will have similar embeddings even though they share no words.\n",
    "\n",
    "**Note:** sentence-transformers does NOT auto-detect Apple MPS - we must pass the device explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load embedding model\n",
    "# Options:\n",
    "# - \"sentence-transformers/all-MiniLM-L6-v2\": Fast, small (80MB), good quality\n",
    "# - \"BAAI/bge-small-en-v1.5\": Better for retrieval, similar size\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(f\"Loading embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Must explicitly pass device for MPS support!\n",
    "embed_model = SentenceTransformer(EMBEDDING_MODEL, device=DEVICE)\n",
    "EMBEDDING_DIM = embed_model.get_sentence_embedding_dimension()\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMO: See how embeddings capture semantic similarity\n",
    "test_sentences = [\n",
    "    \"The engine needs regular oil changes.\",\n",
    "    \"Motor oil should be replaced periodically.\",\n",
    "    \"The Senate convened at noon.\",\n",
    "    \"Congress began its session at midday.\"\n",
    "]\n",
    "\n",
    "test_embeddings = embed_model.encode(test_sentences)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "print(\"Cosine similarity matrix:\")\n",
    "print(\"\\n\" + \" \" * 40 + \"  [0]    [1]    [2]    [3]\")\n",
    "for i, s1 in enumerate(test_sentences):\n",
    "    sims = [cosine_sim(test_embeddings[i], test_embeddings[j]) for j in range(4)]\n",
    "    print(f\"[{i}] {s1[:35]:35} {sims[0]:.3f}  {sims[1]:.3f}  {sims[2]:.3f}  {sims[3]:.3f}\")\n",
    "\n",
    "print(\"\\nâ†’ Notice: [0]-[1] are similar (both about oil), [2]-[3] are similar (both about Congress)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed all chunks - this may take a few minutes for large corpora\n",
    "if all_chunks:\n",
    "    print(f\"Embedding {len(all_chunks)} chunks on {DEVICE}...\")\n",
    "    chunk_texts = [c.text for c in all_chunks]\n",
    "    chunk_embeddings = embed_model.encode(chunk_texts, show_progress_bar=True)\n",
    "    chunk_embeddings = chunk_embeddings.astype('float32')  # FAISS wants float32\n",
    "    print(f\"Embeddings shape: {chunk_embeddings.shape}\")\n",
    "else:\n",
    "    print(\"No chunks to embed - please load documents first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 4: Vector Index (FAISS)\n",
    "\n",
    "FAISS efficiently finds nearest neighbors in high-dimensional spaces.\n",
    "\n",
    "We use a simple **flat index** (brute-force search) which is transparent and works well for up to ~100k vectors. For larger corpora, you'd use approximate methods like IVF or HNSW.\n",
    "\n",
    "**Note:** FAISS GPU support is CUDA-only. On MPS/CPU, we use faiss-cpu (still very fast for <100k vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Create FAISS index\n",
    "# IndexFlatIP = Inner Product (for cosine similarity on normalized vectors)\n",
    "index = faiss.IndexFlatIP(EMBEDDING_DIM)\n",
    "\n",
    "if all_chunks:\n",
    "    # Normalize vectors so inner product = cosine similarity\n",
    "    faiss.normalize_L2(chunk_embeddings)\n",
    "    \n",
    "    # Add vectors to index\n",
    "    index.add(chunk_embeddings)\n",
    "    print(f\"Index built with {index.ntotal} vectors\")\n",
    "else:\n",
    "    print(\"No embeddings to index - please load and embed documents first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 5: Retrieval\n",
    "\n",
    "Now we can search! Given a query, we:\n",
    "1. Embed the query with the same model\n",
    "2. Find the top-k most similar chunks\n",
    "3. Return those chunks as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieve the top-k most relevant chunks for a query.\n",
    "    \n",
    "    Returns: List of (chunk, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embed_model.encode([query]).astype('float32')\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Search\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        if idx != -1:\n",
    "            results.append((all_chunks[idx], float(score)))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval\n",
    "# ============================================\n",
    "# TRY DIFFERENT QUERIES FOR YOUR CORPUS!\n",
    "# ============================================\n",
    "test_query = \"What is the procedure for engine maintenance?\"  # â† Modify this!\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    results = retrieve(test_query, top_k=5)\n",
    "    \n",
    "    print(f\"Query: {test_query}\\n\")\n",
    "    print(\"Top 5 retrieved chunks:\")\n",
    "    for i, (chunk, score) in enumerate(results, 1):\n",
    "        print(f\"\\n[{i}] Score: {score:.4f} | Source: {chunk.source_file}\")\n",
    "        print(f\"    {chunk.text[:200]}...\")\n",
    "else:\n",
    "    print(\"Index is empty - please load, chunk, and embed documents first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 6: Generation (LLM)\n",
    "\n",
    "Now we load a local LLM to generate answers from the retrieved context.\n",
    "\n",
    "**Recommended models:**\n",
    "- `Qwen/Qwen2.5-1.5B-Instruct` - Best instruction following at this size\n",
    "- `Qwen/Qwen2.5-3B-Instruct` - Even better if you have 8GB+ VRAM\n",
    "- `meta-llama/Llama-3.2-1B-Instruct` - Alternative, slightly weaker\n",
    "\n",
    "**Device handling:**\n",
    "- CUDA: Uses `device_map=\"auto\"` and float16\n",
    "- MPS: Loads to CPU first, then moves to MPS with float32\n",
    "- CPU: Uses float32 (slower but works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ============================================\n",
    "# CHOOSE YOUR MODEL\n",
    "# ============================================\n",
    "LLM_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"  # Or try \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "print(f\"Loading LLM: {LLM_MODEL}\")\n",
    "print(f\"Device: {DEVICE}, Dtype: {DTYPE}\")\n",
    "print(\"This may take a few minutes on first run...\\n\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "\n",
    "# Load with appropriate settings for each device type\n",
    "if DEVICE == 'cuda':\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=DTYPE,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"Model loaded on CUDA\")\n",
    "    \n",
    "elif DEVICE == 'mps':\n",
    "    # For MPS, load to CPU first, then move to MPS\n",
    "    # (device_map=\"auto\" doesn't work well with MPS)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL,\n",
    "        torch_dtype=DTYPE,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "    print(\"Model loaded on MPS (Apple Silicon)\")\n",
    "    \n",
    "else:\n",
    "    # CPU\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL,\n",
    "        torch_dtype=DTYPE,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"Model loaded on CPU (this will be slow)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, max_new_tokens: int = 512, temperature: float = 0.3) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the LLM.\n",
    "    \n",
    "    Lower temperature = more focused/deterministic\n",
    "    Higher temperature = more creative/random\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to the correct device\n",
    "    if DEVICE == 'cuda':\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    else:\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True if temperature > 0 else False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 7: The Complete RAG Pipeline\n",
    "\n",
    "Now we put it all together. The **prompt template** is critical - it must instruct the model to use the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RAG prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer the question based ONLY on the information in the context above\n",
    "- If the context doesn't contain enough information to answer, say so\n",
    "- Quote relevant parts of the context to support your answer\n",
    "- Be concise and direct\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "\n",
    "def rag_query(question: str, top_k: int = 5, show_context: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    The complete RAG pipeline:\n",
    "    1. Retrieve relevant chunks\n",
    "    2. Build prompt with context\n",
    "    3. Generate answer\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve\n",
    "    results = retrieve(question, top_k)\n",
    "    \n",
    "    # Format context\n",
    "    context_parts = []\n",
    "    for chunk, score in results:\n",
    "        context_parts.append(f\"[Source: {chunk.source_file}, Relevance: {score:.3f}]\\n{chunk.text}\")\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    \n",
    "    if show_context:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"RETRIEVED CONTEXT:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(context)\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    # Step 2: Build prompt\n",
    "    prompt = PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "    \n",
    "    # Step 3: Generate\n",
    "    answer = generate_response(prompt)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TEST YOUR RAG PIPELINE!\n",
    "# ============================================\n",
    "\n",
    "question = \"What maintenance is required for the engine?\"  # â† Modify for your corpus!\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    print(\"Generating answer...\\n\")\n",
    "    \n",
    "    answer = rag_query(question, top_k=5, show_context=True)\n",
    "    \n",
    "    print(\"ANSWER:\")\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"Pipeline not ready - please complete all previous stages first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiments: Understanding RAG Behavior\n",
    "\n",
    "Now that you have a working pipeline, try these experiments to understand how each component affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 1: Compare WITH vs WITHOUT RAG\n",
    "# ==========================================\n",
    "\n",
    "question = \"What are the specifications for the landing gear?\"  # â† Use a corpus-specific question!\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    # WITHOUT RAG - just ask the model directly\n",
    "    direct_prompt = f\"\"\"Answer this question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    print(\"WITHOUT RAG (model's own knowledge):\")\n",
    "    print(\"-\" * 40)\n",
    "    direct_answer = generate_response(direct_prompt)\n",
    "    print(direct_answer)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    # WITH RAG\n",
    "    print(\"WITH RAG (using retrieved context):\")\n",
    "    print(\"-\" * 40)\n",
    "    rag_answer = rag_query(question, top_k=5)\n",
    "    print(rag_answer)\n",
    "else:\n",
    "    print(\"Please complete the pipeline setup first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 2: Effect of top_k\n",
    "# ==========================================\n",
    "\n",
    "question = \"What safety procedures are required?\"  # â† Use a corpus-specific question!\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    for k in [1, 3, 5, 10]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TOP_K = {k}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        answer = rag_query(question, top_k=k)\n",
    "        print(answer[:500] + \"...\" if len(answer) > 500 else answer)\n",
    "else:\n",
    "    print(\"Please complete the pipeline setup first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 3: Question the corpus CAN'T answer\n",
    "# ==========================================\n",
    "# Does the model admit it doesn't know, or hallucinate?\n",
    "\n",
    "unanswerable_question = \"What is the CEO's favorite color?\"\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    print(f\"Question: {unanswerable_question}\\n\")\n",
    "    answer = rag_query(unanswerable_question, top_k=5, show_context=True)\n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "else:\n",
    "    print(\"Please complete the pipeline setup first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save/Load Your Index\n",
    "\n",
    "For large corpora, you don't want to re-embed every time. Here's how to persist the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_index(filepath: str):\n",
    "    \"\"\"Save FAISS index and chunks to disk.\"\"\"\n",
    "    faiss.write_index(index, f\"{filepath}.faiss\")\n",
    "    with open(f\"{filepath}.chunks\", 'wb') as f:\n",
    "        pickle.dump(all_chunks, f)\n",
    "    print(f\"âœ“ Saved index to {filepath}.faiss\")\n",
    "    print(f\"âœ“ Saved chunks to {filepath}.chunks\")\n",
    "\n",
    "def load_saved_index(filepath: str):\n",
    "    \"\"\"Load FAISS index and chunks from disk.\"\"\"\n",
    "    global index, all_chunks\n",
    "    index = faiss.read_index(f\"{filepath}.faiss\")\n",
    "    with open(f\"{filepath}.chunks\", 'rb') as f:\n",
    "        all_chunks = pickle.load(f)\n",
    "    print(f\"âœ“ Loaded index with {index.ntotal} vectors\")\n",
    "\n",
    "# Save your index\n",
    "if index.ntotal > 0:\n",
    "    save_index(\"my_rag_index\")\n",
    "else:\n",
    "    print(\"No index to save.\")\n",
    "\n",
    "# Later, to load:\n",
    "# load_saved_index(\"my_rag_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "You've built a complete RAG pipeline from scratch! In the next class, we'll:\n",
    "\n",
    "1. **Improve retrieval** with query rewriting and hybrid search\n",
    "2. **Rebuild with LangChain** to see how frameworks abstract these steps\n",
    "3. **Evaluate systematically** with test questions and metrics\n",
    "\n",
    "### Exercises to try:\n",
    "- Vary chunk size (256, 512, 1024) and measure retrieval quality\n",
    "- Try a different embedding model (`BAAI/bge-small-en-v1.5`)\n",
    "- Try a larger LLM (`Qwen/Qwen2.5-3B-Instruct`) and compare answer quality\n",
    "- Ask questions that require combining information from multiple chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: Device Information\n",
    "\n",
    "Run this cell to see detailed information about your compute environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_device_info():\n",
    "    \"\"\"Print detailed information about available compute devices.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DEVICE INFORMATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nEnvironment: {ENVIRONMENT}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # CUDA\n",
    "    print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # MPS\n",
    "    print(f\"\\nMPS available: {torch.backends.mps.is_available()}\")\n",
    "    print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "    \n",
    "    # Current selection\n",
    "    print(f\"\\nâ†’ Selected device: {DEVICE}\")\n",
    "    print(f\"â†’ Selected dtype: {DTYPE}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print_device_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
