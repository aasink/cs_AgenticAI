{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual RAG Pipeline: Mechanisms First\n",
    "\n",
    "This notebook builds a Retrieval-Augmented Generation (RAG) pipeline from scratch.\n",
    "You'll see every step explicitly before we move to frameworks like LangChain.\n",
    "\n",
    "**Works on:** Google Colab, Local Jupyter (Mac/Windows/Linux)\n",
    "\n",
    "**Pipeline Overview:**\n",
    "```\n",
    "Documents ‚Üí Chunking ‚Üí Embedding ‚Üí Index (FAISS)\n",
    "                                        ‚Üì\n",
    "User Query ‚Üí Embed Query ‚Üí Similarity Search ‚Üí Top-K Chunks\n",
    "                                                    ‚Üì\n",
    "                                        Prompt Assembly ‚Üí LLM ‚Üí Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO ‚Äî Topic 5 RAG Course Project Checklist\n",
    "\n",
    "- **Exercise 0:** Set-up ‚Äî Get notebook running; unzip Corpora.zip. Use PDFs from `Corpora/<corpus>/pdf_embedded/`.\n",
    "- **Exercise 1:** Open model RAG vs no RAG ‚Äî Compare Qwen 2.5 1.5B with/without RAG on Model T manual and Congressional Record.\n",
    "- **Exercise 2:** Open model + RAG vs large model ‚Äî Run GPT-4o Mini with no tools on same queries.\n",
    "- **Exercise 3:** Open model + RAG vs frontier chat ‚Äî Compare local Qwen+RAG vs GPT-4/Claude (web).\n",
    "- **Exercise 4:** Effect of top-K ‚Äî Test k = 1, 3, 5, 10, 20.\n",
    "- **Exercise 5:** Unanswerable questions ‚Äî Off-topic, related-but-missing, false premise.\n",
    "- **Exercise 6:** Query phrasing sensitivity ‚Äî Same question in 5+ phrasings.\n",
    "- **Exercise 7:** Chunk overlap ‚Äî Re-chunk with overlap 0, 64, 128, 256.\n",
    "- **Exercise 8:** Chunk size ‚Äî Chunk at 128, 256, 512, 1024, 2048.\n",
    "- **Exercise 9:** Retrieval score analysis ‚Äî 10 queries, top-10 chunks, score distribution.\n",
    "- **Exercise 10:** Prompt template variations ‚Äî Minimal, strict grounding, citation, permissive, structured.\n",
    "- **Exercise 11:** Failure mode catalog ‚Äî Computation, temporal, comparison, ambiguous, multi-hop, etc.\n",
    "- **Exercise 12:** Cross-document synthesis ‚Äî Questions needing multiple chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages and detect our compute environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# On Colab, these install quickly. Locally, you may already have them.\n",
    "!pip install -q torch transformers sentence-transformers faiss-cpu pymupdf accelerate ipyfilechooser\n",
    "# For Exercise 2 (GPT-4o Mini): !pip install -q openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using Apple Silicon GPU (MPS)\n",
      "  Note: Using float32 (faster than float16 on Apple Silicon)\n",
      "\n",
      "Environment: LOCAL\n",
      "Device: mps, Dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT AND DEVICE DETECTION\n",
    "# =============================================================================\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Enable MPS fallback for any PyTorch operations not yet implemented on Metal\n",
    "# This MUST be set before importing torch\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "def detect_environment() -> str:\n",
    "    \"\"\"Detect if we're running on Colab or locally.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except ImportError:\n",
    "        return 'local'\n",
    "\n",
    "def get_device() -> Tuple[str, torch.dtype]:\n",
    "    \"\"\"\n",
    "    Detect the best available compute device.\n",
    "    \n",
    "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (device_string, recommended_dtype)\n",
    "        \n",
    "    Notes:\n",
    "        - CUDA: Use float16 for memory efficiency (Tensor Cores optimize this)\n",
    "        - MPS: Use float32 - Apple Silicon doesn't have the same float16 \n",
    "               optimizations as NVIDIA, and float32 is often faster\n",
    "        - CPU: Use float32 (float16 not well supported on CPU)\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        dtype = torch.float16\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"‚úì Using CUDA GPU: {device_name} ({memory_gb:.1f} GB)\")\n",
    "        \n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = 'mps'\n",
    "        dtype = torch.float32  # float32 is often faster on Apple Silicon!\n",
    "        print(\"‚úì Using Apple Silicon GPU (MPS)\")\n",
    "        print(\"  Note: Using float32 (faster than float16 on Apple Silicon)\")\n",
    "        \n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        dtype = torch.float32\n",
    "        print(\"‚ö† Using CPU (no GPU detected)\")\n",
    "        print(\"  Tip: For faster processing, use a machine with a GPU\")\n",
    "    \n",
    "    return device, dtype\n",
    "\n",
    "# Detect environment and device\n",
    "ENVIRONMENT = detect_environment()\n",
    "DEVICE, DTYPE = get_device()\n",
    "\n",
    "print(f\"\\nEnvironment: {ENVIRONMENT.upper()}\")\n",
    "print(f\"Device: {DEVICE}, Dtype: {DTYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Your Documents\n",
    "\n",
    "**Cell 1:** Configure your document source and select/upload files\n",
    "- **Local Jupyter**: Use the folder picker, then run Cell 2\n",
    "- **Colab + Upload**: Files upload immediately (blocking), then run Cell 2\n",
    "- **Colab + Drive**: Set `USE_GOOGLE_DRIVE = True`, mounts Drive and shows picker, then run Cell 2\n",
    "\n",
    "**Cell 2:** Confirms selection and lists documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "\n",
      "üìÅ Select your documents folder below, then run Cell 2:\n",
      "   (The picker is non-blocking - select BEFORE running the next cell)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc40ad2bc4fd4ddd8026f56b3a87649a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileChooser(path='/Users/nup', filename='', title='Select your documents folder', show_hidden=False, select_de‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: SELECT DOCUMENT SOURCE\n",
    "# =============================================================================\n",
    "# This cell either:\n",
    "#   - Shows a folder picker (Local or Colab+Drive) - NON-BLOCKING\n",
    "#   - Shows an upload dialog (Colab+Upload) - BLOCKING\n",
    "#\n",
    "# If a folder picker is shown, SELECT YOUR FOLDER BEFORE running Cell 2.\n",
    "# The picker widget is non-blocking, so the code continues before you select.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------- COLAB USERS: CONFIGURE HERE -------------\n",
    "USE_GOOGLE_DRIVE = False  # Set to True to use Google Drive instead of uploading\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Default folder: use Corpora from course project (unzip Corpora.zip first).\n",
    "_folder_default = Path(\"Corpora/ModelTService\")\n",
    "DOC_FOLDER = str(_folder_default) if _folder_default.exists() else \"documents\"\n",
    "folder_chooser = None  # Will hold the picker widget if used\n",
    "\n",
    "if ENVIRONMENT == 'colab':\n",
    "    if USE_GOOGLE_DRIVE:\n",
    "        # ----- COLAB + GOOGLE DRIVE -----\n",
    "        # Mount Drive first, then show folder picker\n",
    "        from google.colab import drive\n",
    "        print(\"Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"‚úì Google Drive mounted\\n\")\n",
    "        \n",
    "        # Now show folder picker for the Drive\n",
    "        try:\n",
    "            from ipyfilechooser import FileChooser\n",
    "            \n",
    "            folder_chooser = FileChooser(\n",
    "                path='/content/drive/MyDrive',\n",
    "                title='Select your documents folder in Google Drive',\n",
    "                show_only_dirs=True,\n",
    "                select_default=True\n",
    "            )\n",
    "            print(\"üìÅ Select your documents folder below, then run Cell 2:\")\n",
    "            print(\"   (The picker is non-blocking - select BEFORE running the next cell)\")\n",
    "            display(folder_chooser)\n",
    "            \n",
    "        except ImportError:\n",
    "            # Fallback: manual path entry\n",
    "            print(\"Folder picker not available.\")\n",
    "            print(\"Edit DOC_FOLDER below with your Google Drive path, then run Cell 2:\")\n",
    "            DOC_FOLDER = '/content/drive/MyDrive/your_documents_folder'  # ‚Üê Edit this!\n",
    "            print(f\"  DOC_FOLDER = '{DOC_FOLDER}'\")\n",
    "    else:\n",
    "        # ----- COLAB + UPLOAD -----\n",
    "        # Upload dialog blocks until complete, so DOC_FOLDER is ready when done\n",
    "        from google.colab import files\n",
    "        os.makedirs(DOC_FOLDER, exist_ok=True)\n",
    "        \n",
    "        print(\"Upload your documents (PDF, TXT, or MD):\")\n",
    "        print(\"(This dialog blocks until upload is complete)\\n\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        for filename in uploaded.keys():\n",
    "            os.rename(filename, f'{DOC_FOLDER}/{filename}')\n",
    "            print(f\"  ‚úì Saved: {DOC_FOLDER}/{filename}\")\n",
    "        \n",
    "        print(f\"\\n‚úì Upload complete. Run Cell 2 to continue.\")\n",
    "\n",
    "else:\n",
    "    # ----- LOCAL JUPYTER -----\n",
    "    # Show folder picker\n",
    "    print(\"Running locally\\n\")\n",
    "    \n",
    "    try:\n",
    "        from ipyfilechooser import FileChooser\n",
    "        \n",
    "        folder_chooser = FileChooser(\n",
    "            path=str(Path.home()),\n",
    "            title='Select your documents folder',\n",
    "            show_only_dirs=True,\n",
    "            select_default=True\n",
    "        )\n",
    "        print(\"üìÅ Select your documents folder below, then run Cell 2:\")\n",
    "        print(\"   (The picker is non-blocking - select BEFORE running the next cell)\")\n",
    "        display(folder_chooser)\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback: manual path entry\n",
    "        print(\"Folder picker not available (ipyfilechooser not installed).\")\n",
    "        print(f\"\\nUsing default folder: {Path(DOC_FOLDER).absolute()}\")\n",
    "        print(\"\\nTo use a different folder, edit DOC_FOLDER in this cell:\")\n",
    "        print(\"  DOC_FOLDER = '/path/to/your/documents'\")\n",
    "        os.makedirs(DOC_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using selected folder: /Users/nup/Documents/GradSchool/cs_AgenticAI/Topic5_RAG/Corpora/ModelTService\n",
      "‚úì Folder set: /Users/nup/Documents/GradSchool/cs_AgenticAI/Topic5_RAG/Corpora/ModelTService\n",
      "  Run the next cells to load, chunk, and index documents.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: CONFIRM SELECTION AND LIST DOCUMENTS\n",
    "# =============================================================================\n",
    "# If you used a folder picker above, make sure you selected a folder\n",
    "# BEFORE running this cell. The picker is non-blocking.\n",
    "# =============================================================================\n",
    "\n",
    "# Read selection from folder picker (if one was used)\n",
    "if folder_chooser is not None and folder_chooser.selected_path:\n",
    "    DOC_FOLDER = folder_chooser.selected_path\n",
    "    print(f\"‚úì Using selected folder: {DOC_FOLDER}\")\n",
    "elif folder_chooser is not None:\n",
    "    print(\"‚ö† No folder selected in picker!\")\n",
    "    print(\"  Please go back to Cell 1, select a folder, then run this cell again.\")\n",
    "else:\n",
    "    # No picker used (upload or manual path)\n",
    "    print(f\"‚úì Using folder: {DOC_FOLDER}\")\n",
    "\n",
    "# Confirm folder (listing skipped for speed)\n",
    "doc_path = Path(DOC_FOLDER)\n",
    "if doc_path.exists():\n",
    "    print(f\"‚úì Folder set: {doc_path.absolute()}\")\n",
    "    print(\"  Run the next cells to load, chunk, and index documents.\")\n",
    "else:\n",
    "    print(f\"‚ö† Folder not found: {DOC_FOLDER}\")\n",
    "    print(\"  Please set DOC_FOLDER in the previous cell and run it again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 1: Document Loading\n",
    "\n",
    "We need to extract text from our documents. For PDFs with embedded text,\n",
    "PyMuPDF (fitz) reads the text layer directly - no OCR needed.\n",
    "\n",
    "**Corpora:** Use PDFs from `Corpora/<name>/pdf_embedded/`. The `.txt` files in `txt/` are for checking retrieval vs OCR issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 (and reuse): Official query lists. Reference: CR Jan 13, 20, 21, 23, 2026.\n",
    "QUERIES_MODEL_T = [\n",
    "    \"How do I adjust the carburetor on a Model T?\",\n",
    "    \"What is the correct spark plug gap for a Model T Ford?\",\n",
    "    \"How do I fix a slipping transmission band?\",\n",
    "    \"What oil should I use in a Model T engine?\",\n",
    "]\n",
    "QUERIES_CR = [\n",
    "    \"What did Mr. Flood have to say about Mayor David Black in Congress on January 13, 2026?\",\n",
    "    \"What mistake did Elise Stefanik make in Congress on January 23, 2026?\",\n",
    "    \"What is the purpose of the Main Street Parity Act?\",\n",
    "    \"Who in Congress has spoken for and against funding of pregnancy centers?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from typing import List, Tuple\n",
    "\n",
    "def load_text_file(filepath: str) -> str:\n",
    "    \"\"\"Load a plain text file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def load_pdf_file(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF with embedded text.\n",
    "    \n",
    "    PyMuPDF reads the text layer directly.\n",
    "    For scanned PDFs without embedded text, you'd need OCR.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(filepath)\n",
    "    text_parts = []\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        if text.strip():\n",
    "            # Add page marker for debugging/citation\n",
    "            text_parts.append(f\"\\n[Page {page_num + 1}]\\n{text}\")\n",
    "    \n",
    "    doc.close()\n",
    "    return \"\\n\".join(text_parts)\n",
    "\n",
    "\n",
    "def load_documents(doc_folder: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Load all documents from a folder. Returns list of (filename, content).\"\"\"\n",
    "    documents = []\n",
    "    folder = Path(doc_folder)\n",
    "    \n",
    "    for filepath in folder.rglob(\"*\"):\n",
    "        try:\n",
    "            if not filepath.is_file():\n",
    "                continue\n",
    "        except OSError:\n",
    "            continue\n",
    "        if filepath.suffix.lower() not in ('.pdf', '.txt', '.md', '.text'):\n",
    "            continue\n",
    "        try:\n",
    "            if filepath.suffix.lower() == '.pdf':\n",
    "                    content = load_pdf_file(str(filepath))\n",
    "            elif filepath.suffix.lower() in ['.txt', '.md', '.text']:\n",
    "                    content = load_text_file(str(filepath))\n",
    "            else:\n",
    "                    continue\n",
    "                \n",
    "            if content.strip():\n",
    "                    documents.append((filepath.name, content))\n",
    "                    print(f\"‚úì Loaded: {filepath.name} ({len(content):,} chars)\")\n",
    "        except Exception as e:\n",
    "                print(f\"‚úó Error loading {filepath}: {e}\")\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded: ModelT-61-62.txt (201 chars)\n",
      "‚úì Loaded: ModelT-11-20.txt (19,009 chars)\n",
      "‚úì Loaded: ModelT-31-40.txt (12,194 chars)\n",
      "‚úì Loaded: Ford-Model-T-Man-1919.txt (95,574 chars)\n",
      "‚úì Loaded: ModelT-51-60.txt (14,168 chars)\n",
      "‚úì Loaded: ModelT-21-30.txt (17,050 chars)\n",
      "‚úì Loaded: ModelT-41-50.txt (14,264 chars)\n",
      "‚úì Loaded: ModelT-01-10.txt (18,676 chars)\n",
      "‚úì Loaded: ModelT-31-40-ocr.pdf (12,201 chars)\n",
      "‚úì Loaded: Ford-Model-T-Man-1919-ocr.pdf (95,517 chars)\n",
      "‚úì Loaded: ModelT-21-30-ocr.pdf (17,025 chars)\n",
      "‚úì Loaded: ModelT-61-62-ocr.pdf (204 chars)\n",
      "‚úì Loaded: ModelT-51-60-ocr.pdf (14,107 chars)\n",
      "‚úì Loaded: ModelT-41-50-ocr.pdf (14,270 chars)\n",
      "‚úì Loaded: ModelT-01-10-ocr.pdf (18,658 chars)\n",
      "‚úì Loaded: ModelT-11-20-ocr.pdf (19,003 chars)\n",
      "\n",
      "Loaded 16 documents\n"
     ]
    }
   ],
   "source": [
    "# Load your documents\n",
    "documents = load_documents(DOC_FOLDER)\n",
    "print(f\"\\nLoaded {len(documents)} documents\")\n",
    "\n",
    "if len(documents) == 0:\n",
    "    print(\"\\n‚ö† No documents loaded! Please add PDF or TXT files to the documents folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First document: ModelT-61-62.txt\n",
      "Total length: 201 characters\n",
      "\n",
      "First 1000 characters:\n",
      "----------------------------------------\n",
      "What It Is For\n",
      "How to Disconnect\n",
      "\n",
      "OPERATION\n",
      "Alustments‚ÄîWhat to Do vere... cong\n",
      "\n",
      "Model T Truck\n",
      "\n",
      "‚ÄòWorm, How Removed rar bo eee ne ceeaee ba eeeeeces reer er rrr\n",
      "\n",
      "The Ford Starting and Highting System\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect a document to verify loading worked\n",
    "if documents:\n",
    "    filename, content = documents[0]\n",
    "    print(f\"First document: {filename}\")\n",
    "    print(f\"Total length: {len(content):,} characters\")\n",
    "    print(f\"\\nFirst 1000 characters:\\n{'-'*40}\")\n",
    "    print(content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 2: Chunking\n",
    "\n",
    "Documents need to be split into pieces small enough to be relevant but large enough to carry meaning.\n",
    "\n",
    "**Why overlap?** If a key sentence sits right at a chunk boundary, splitting without overlap might cut it in half. Overlap ensures that information near boundaries appears intact in at least one chunk.\n",
    "\n",
    "**Experiment:** Try different chunk sizes (256, 512, 1024) and see how it affects retrieval!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"A chunk of text with metadata for tracing back to source.\"\"\"\n",
    "    text: str\n",
    "    source_file: str\n",
    "    chunk_index: int\n",
    "    start_char: int\n",
    "    end_char: int\n",
    "\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    source_file: str,\n",
    "    chunk_size: int = 512,\n",
    "    chunk_overlap: int = 128\n",
    ") -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \n",
    "    We try to break at sentence or paragraph boundaries\n",
    "    to avoid cutting mid-thought.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    chunk_index = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        # Try to break at a good boundary\n",
    "        if end < len(text):\n",
    "            # Look for paragraph break first\n",
    "            para_break = text.rfind('\\n\\n', start + chunk_size // 2, end)\n",
    "            if para_break != -1:\n",
    "                end = para_break + 2\n",
    "            else:\n",
    "                # Look for sentence break\n",
    "                sentence_break = text.rfind('. ', start + chunk_size // 2, end)\n",
    "                if sentence_break != -1:\n",
    "                    end = sentence_break + 2\n",
    "        \n",
    "        chunk_text_str = text[start:end].strip()\n",
    "        \n",
    "        if chunk_text_str:\n",
    "            chunks.append(Chunk(\n",
    "                text=chunk_text_str,\n",
    "                source_file=source_file,\n",
    "                chunk_index=chunk_index,\n",
    "                start_char=start,\n",
    "                end_char=end\n",
    "            ))\n",
    "            chunk_index += 1\n",
    "        \n",
    "        # Move forward, accounting for overlap\n",
    "        start = end - chunk_overlap\n",
    "        if chunks and start <= chunks[-1].start_char:\n",
    "            start = end  # Safety: ensure progress\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelT-61-62.txt: 1 chunks\n",
      "ModelT-11-20.txt: 66 chunks\n",
      "ModelT-31-40.txt: 44 chunks\n",
      "Ford-Model-T-Man-1919.txt: 326 chunks\n",
      "ModelT-51-60.txt: 46 chunks\n",
      "ModelT-21-30.txt: 56 chunks\n",
      "ModelT-41-50.txt: 51 chunks\n",
      "ModelT-01-10.txt: 64 chunks\n",
      "ModelT-31-40-ocr.pdf: 44 chunks\n",
      "Ford-Model-T-Man-1919-ocr.pdf: 316 chunks\n",
      "ModelT-21-30-ocr.pdf: 56 chunks\n",
      "ModelT-61-62-ocr.pdf: 1 chunks\n",
      "ModelT-51-60-ocr.pdf: 44 chunks\n",
      "ModelT-41-50-ocr.pdf: 47 chunks\n",
      "ModelT-01-10-ocr.pdf: 63 chunks\n",
      "ModelT-11-20-ocr.pdf: 61 chunks\n",
      "\n",
      "Total: 1286 chunks\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EXPERIMENT: Try different chunk sizes!\n",
    "# ============================================\n",
    "CHUNK_SIZE = 512      # Try: 256, 512, 1024\n",
    "CHUNK_OVERLAP = 128   # Try: 64, 128, 256\n",
    "# For Ex 7/8 use rebuild_pipeline() ‚Äî see cell after FAISS index.\n",
    "\n",
    "# Chunk all documents\n",
    "all_chunks = []\n",
    "for filename, content in documents:\n",
    "    doc_chunks = chunk_text(content, filename, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "    all_chunks.extend(doc_chunks)\n",
    "    print(f\"{filename}: {len(doc_chunks)} chunks\")\n",
    "\n",
    "print(f\"\\nTotal: {len(all_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunks:\n",
      "\n",
      "============================================================\n",
      "Chunk 0 from ModelT-61-62.txt\n",
      "============================================================\n",
      "What It Is For\n",
      "How to Disconnect\n",
      "\n",
      "OPERATION\n",
      "Alustments‚ÄîWhat to Do vere... cong\n",
      "\n",
      "Model T Truck\n",
      "\n",
      "‚ÄòWorm, How Removed rar bo eee ne ceeaee ba eeeeeces reer er rrr\n",
      "\n",
      "The Ford Starting and Highting System\n",
      "\n",
      "============================================================\n",
      "Chunk 53 from ModelT-01-10.txt\n",
      "============================================================\n",
      "in that condition.\n",
      "\n",
      "7‚Äù\n",
      "\n",
      ": to _ es\n",
      "Pa toy a oa a of\n",
      "\n",
      "\n",
      "_ The Ford Engine\n",
      "\n",
      "What ts the principle of the\n",
      "\n",
      "gasoline-driven Engine?\n",
      "\n",
      "Gasoline when mixed with air and compressed is highly explo: fm explosion\n",
      "ia @ violent expansion caused by instantaneous combustio afined. gases.\n",
      "In the gasoline engine the ...\n",
      "\n",
      "============================================================\n",
      "Chunk 60 from ModelT-11-20-ocr.pdf\n",
      "============================================================\n",
      "install a new one.\n",
      "\n",
      "\n",
      "[Page 10]\n",
      "iin Ta i Se\n",
      "nt i\n",
      "This cut illustrates the principle of Ford Carburetion.\n",
      "¬ª\n",
      "(Cut No. 7)\n"
     ]
    }
   ],
   "source": [
    "# Inspect some chunks\n",
    "if all_chunks:\n",
    "    print(\"Sample chunks:\")\n",
    "    indices_to_show = [0, len(all_chunks)//2, -1] if len(all_chunks) > 2 else range(len(all_chunks))\n",
    "    for i in indices_to_show:\n",
    "        chunk = all_chunks[i]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Chunk {chunk.chunk_index} from {chunk.source_file}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(chunk.text[:300] + \"...\" if len(chunk.text) > 300 else chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 3: Embedding\n",
    "\n",
    "Embeddings map text to dense vectors where **semantic similarity = geometric proximity**.\n",
    "\n",
    "A sentence about \"cardiac arrest\" and one about \"heart attack\" will have similar embeddings even though they share no words.\n",
    "\n",
    "**Note:** sentence-transformers does NOT auto-detect Apple MPS - we must pass the device explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Device: mps\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load embedding model\n",
    "# Options:\n",
    "# - \"sentence-transformers/all-MiniLM-L6-v2\": Fast, small (80MB), good quality\n",
    "# - \"BAAI/bge-small-en-v1.5\": Better for retrieval, similar size\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(f\"Loading embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Must explicitly pass device for MPS support!\n",
    "embed_model = SentenceTransformer(EMBEDDING_MODEL, device=DEVICE)\n",
    "EMBEDDING_DIM = embed_model.get_sentence_embedding_dimension()\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity matrix:\n",
      "\n",
      "                                          [0]    [1]    [2]    [3]\n",
      "[0] The engine needs regular oil change 1.000  0.728  -0.045  -0.032\n",
      "[1] Motor oil should be replaced period 0.728  1.000  0.014  0.035\n",
      "[2] The Senate convened at noon.        -0.045  0.014  1.000  0.684\n",
      "[3] Congress began its session at midda -0.032  0.035  0.684  1.000\n",
      "\n",
      "‚Üí Notice: [0]-[1] are similar (both about oil), [2]-[3] are similar (both about Congress)\n"
     ]
    }
   ],
   "source": [
    "# DEMO: See how embeddings capture semantic similarity\n",
    "test_sentences = [\n",
    "    \"The engine needs regular oil changes.\",\n",
    "    \"Motor oil should be replaced periodically.\",\n",
    "    \"The Senate convened at noon.\",\n",
    "    \"Congress began its session at midday.\"\n",
    "]\n",
    "\n",
    "test_embeddings = embed_model.encode(test_sentences)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "print(\"Cosine similarity matrix:\")\n",
    "print(\"\\n\" + \" \" * 40 + \"  [0]    [1]    [2]    [3]\")\n",
    "for i, s1 in enumerate(test_sentences):\n",
    "    sims = [cosine_sim(test_embeddings[i], test_embeddings[j]) for j in range(4)]\n",
    "    print(f\"[{i}] {s1[:35]:35} {sims[0]:.3f}  {sims[1]:.3f}  {sims[2]:.3f}  {sims[3]:.3f}\")\n",
    "\n",
    "print(\"\\n‚Üí Notice: [0]-[1] are similar (both about oil), [2]-[3] are similar (both about Congress)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1286 chunks on mps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbac8ec22f3b4767b09e41cc26b46d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1286, 384)\n"
     ]
    }
   ],
   "source": [
    "# Embed all chunks - this may take a few minutes for large corpora\n",
    "if all_chunks:\n",
    "    print(f\"Embedding {len(all_chunks)} chunks on {DEVICE}...\")\n",
    "    chunk_texts = [c.text for c in all_chunks]\n",
    "    chunk_embeddings = embed_model.encode(chunk_texts, show_progress_bar=True)\n",
    "    chunk_embeddings = chunk_embeddings.astype('float32')  # FAISS wants float32\n",
    "    print(f\"Embeddings shape: {chunk_embeddings.shape}\")\n",
    "else:\n",
    "    print(\"No chunks to embed - please load documents first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 4: Vector Index (FAISS)\n",
    "\n",
    "FAISS efficiently finds nearest neighbors in high-dimensional spaces.\n",
    "\n",
    "We use a simple **flat index** (brute-force search) which is transparent and works well for up to ~100k vectors. For larger corpora, you'd use approximate methods like IVF or HNSW.\n",
    "\n",
    "**Note:** FAISS GPU support is CUDA-only. On MPS/CPU, we use faiss-cpu (still very fast for <100k vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built with 1286 vectors\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Create FAISS index\n",
    "# IndexFlatIP = Inner Product (for cosine similarity on normalized vectors)\n",
    "index = faiss.IndexFlatIP(EMBEDDING_DIM)\n",
    "\n",
    "if all_chunks:\n",
    "    # Normalize vectors so inner product = cosine similarity\n",
    "    faiss.normalize_L2(chunk_embeddings)\n",
    "    \n",
    "    # Add vectors to index\n",
    "    index.add(chunk_embeddings)\n",
    "    print(f\"Index built with {index.ntotal} vectors\")\n",
    "else:\n",
    "    print(\"No embeddings to index - please load and embed documents first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 5: Retrieval\n",
    "\n",
    "Now we can search! Given a query, we:\n",
    "1. Embed the query with the same model\n",
    "2. Find the top-k most similar chunks\n",
    "3. Return those chunks as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for Exercises 7 & 8: rebuild chunks + index with different chunk_size / chunk_overlap.\n",
    "def rebuild_pipeline(chunk_size: int = 512, chunk_overlap: int = 128):\n",
    "    \"\"\"Re-chunk documents, re-embed, and rebuild FAISS index. Updates global all_chunks and index.\"\"\"\n",
    "    global all_chunks, index\n",
    "    all_chunks = []\n",
    "    for filename, content in documents:\n",
    "        all_chunks.extend(chunk_text(content, filename, chunk_size=chunk_size, chunk_overlap=chunk_overlap))\n",
    "    chunk_embeddings = embed_model.encode([c.text for c in all_chunks], show_progress_bar=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(chunk_embeddings)\n",
    "    index = faiss.IndexFlatIP(EMBEDDING_DIM)\n",
    "    index.add(chunk_embeddings)\n",
    "    print(f\"Rebuilt: {len(all_chunks)} chunks, chunk_size={chunk_size}, chunk_overlap={chunk_overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieve the top-k most relevant chunks for a query.\n",
    "    \n",
    "    Returns: List of (chunk, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embed_model.encode([query]).astype('float32')\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Search\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        if idx != -1:\n",
    "            results.append((all_chunks[idx], float(score)))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the procedure for engine maintenance?\n",
      "\n",
      "Top 5 retrieved chunks:\n",
      "\n",
      "[1] Score: 0.4715 | Source: ModelT-01-10.txt\n",
      "    ake a practice of taking care of every repair '\n",
      "adjustment as soon as its necessity is discovered. This attention requires but\n",
      "\n",
      "little time and may. avoid delay or possible accident on the road. We ai...\n",
      "\n",
      "[2] Score: 0.4715 | Source: Ford-Model-T-Man-1919.txt\n",
      "    ake a practice of taking care of every repair '\n",
      "adjustment as soon as its necessity is discovered. This attention requires but\n",
      "\n",
      "little time and may. avoid delay or possible accident on the road. We ai...\n",
      "\n",
      "[3] Score: 0.4531 | Source: Ford-Model-T-Man-1919-ocr.pdf\n",
      "    o far.\n",
      "6. Gas mixture too rich.\n",
      "7. Water circulation retarded ‚Äòby\n",
      "sediment in radiator. .\n",
      "8 Dirty spark plugs.\n",
      "ENGINE KNOCKS\n",
      "1. Carbon deposit on piston heads.\n",
      "2. Loose connecting rod bearing.\n",
      "3. Loos...\n",
      "\n",
      "[4] Score: 0.4486 | Source: Ford-Model-T-Man-1919.txt\n",
      "    far.\n",
      "6. Gas mixture too rich.\n",
      "\n",
      "7. Water circulation retarded ‚Äòby\n",
      "sediment in radiator. .\n",
      "\n",
      "8 Dirty spark plugs.\n",
      "\n",
      "ENGINE KNOCKS\n",
      "\n",
      "1. Carbon deposit on piston heads.\n",
      "2. Loose connecting rod bearing.\n",
      "3. Lo...\n",
      "\n",
      "[5] Score: 0.4378 | Source: Ford-Model-T-Man-1919-ocr.pdf\n",
      "    y...\n",
      "Water in gasoline.\n",
      "3. Flooded carburetor.\n",
      "4. Dirt in carburetor or feed pips:\n",
      "5. Magneto wire foose at either\n",
      "terminal.\n",
      "6. Magneto contact point obstructed.\n",
      "Overheated‚Äîaccount lack of oil\n",
      "or wate...\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval\n",
    "# ============================================\n",
    "# TRY DIFFERENT QUERIES FOR YOUR CORPUS!\n",
    "# ============================================\n",
    "test_query = \"What is the procedure for engine maintenance?\"  # ‚Üê Modify this!\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    results = retrieve(test_query, top_k=5)\n",
    "    \n",
    "    print(f\"Query: {test_query}\\n\")\n",
    "    print(\"Top 5 retrieved chunks:\")\n",
    "    for i, (chunk, score) in enumerate(results, 1):\n",
    "        print(f\"\\n[{i}] Score: {score:.4f} | Source: {chunk.source_file}\")\n",
    "        print(f\"    {chunk.text[:200]}...\")\n",
    "else:\n",
    "    print(\"Index is empty - please load, chunk, and embed documents first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 6: Generation (LLM)\n",
    "\n",
    "Now we load a local LLM to generate answers from the retrieved context.\n",
    "\n",
    "**Recommended models:**\n",
    "- `Qwen/Qwen2.5-1.5B-Instruct` - Best instruction following at this size\n",
    "- `Qwen/Qwen2.5-3B-Instruct` - Even better if you have 8GB+ VRAM\n",
    "- `meta-llama/Llama-3.2-1B-Instruct` - Alternative, slightly weaker\n",
    "\n",
    "**Device handling:**\n",
    "- CUDA: Uses `device_map=\"auto\"` and float16\n",
    "- MPS: Loads to CPU first, then moves to MPS with float32\n",
    "- CPU: Uses float32 (slower but works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Device: mps, Dtype: torch.float32\n",
      "This may take a few minutes on first run...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on MPS (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ============================================\n",
    "# CHOOSE YOUR MODEL\n",
    "# ============================================\n",
    "LLM_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"  # Or try \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "print(f\"Loading LLM: {LLM_MODEL}\")\n",
    "print(f\"Device: {DEVICE}, Dtype: {DTYPE}\")\n",
    "print(\"This may take a few minutes on first run...\\n\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "\n",
    "# Load with appropriate settings for each device type\n",
    "if DEVICE == 'cuda':\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=DTYPE,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"Model loaded on CUDA\")\n",
    "    \n",
    "elif DEVICE == 'mps':\n",
    "    # For MPS, load to CPU first, then move to MPS\n",
    "    # (device_map=\"auto\" doesn't work well with MPS)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL,\n",
    "        torch_dtype=DTYPE,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "    print(\"Model loaded on MPS (Apple Silicon)\")\n",
    "    \n",
    "else:\n",
    "    # CPU\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL,\n",
    "        torch_dtype=DTYPE,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"Model loaded on CPU (this will be slow)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, max_new_tokens: int = 512, temperature: float = 0.3) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the LLM.\n",
    "    \n",
    "    Lower temperature = more focused/deterministic\n",
    "    Higher temperature = more creative/random\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to the correct device\n",
    "    if DEVICE == 'cuda':\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    else:\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True if temperature > 0 else False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 7: The Complete RAG Pipeline\n",
    "\n",
    "Now we put it all together. The **prompt template** is critical - it must instruct the model to use the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RAG prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer the question based ONLY on the information in the context above\n",
    "- If the context doesn't contain enough information to answer, say so\n",
    "- Quote relevant parts of the context to support your answer\n",
    "- Be concise and direct\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "\n",
    "def direct_query(question: str, max_new_tokens: int = 512) -> str:\n",
    "    \"\"\"Ask the LLM directly with no retrieved context (for RAG vs no-RAG comparison).\"\"\"\n",
    "    prompt = f\"\"\"Answer this question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return generate_response(prompt, max_new_tokens=max_new_tokens)\n",
    "\n",
    "def rag_query(question: str, top_k: int = 5, show_context: bool = False, prompt_template: str = None) -> str:\n",
    "    \"\"\"The complete RAG pipeline. prompt_template: custom template for Exercise 10.\"\"\"\n",
    "    # Step 1: Retrieve\n",
    "    results = retrieve(question, top_k)\n",
    "    \n",
    "    # Format context\n",
    "    context_parts = []\n",
    "    for chunk, score in results:\n",
    "        context_parts.append(f\"[Source: {chunk.source_file}, Relevance: {score:.3f}]\\n{chunk.text}\")\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    \n",
    "    if show_context:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"RETRIEVED CONTEXT:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(context)\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    # Step 2: Build prompt (use custom template if provided)\n",
    "    template = prompt_template if prompt_template is not None else PROMPT_TEMPLATE\n",
    "    prompt = template.format(context=context, question=question)\n",
    "    \n",
    "    # Step 3: Generate\n",
    "    answer = generate_response(prompt)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What maintenance is required for the engine?\n",
      "\n",
      "Generating answer...\n",
      "\n",
      "============================================================\n",
      "RETRIEVED CONTEXT:\n",
      "============================================================\n",
      "[Source: ModelT-01-10-ocr.pdf, Relevance: 0.494]\n",
      ".\n",
      "entire Ford orgenization is interested in keeping every individual Ford. car\n",
      "constant operation, at the lowest possible cost. We have known of much d√©\n",
      "done to miany cars by unskilled repair men.\n",
      "What attention. Z\n",
      "does the Car need? Answer No. vA\n",
      "mer that a new machine requires more careful attention during the fiz\n",
      "few √© itis being driven than efter the parts have become thoroughly ‚Äúworked!\n",
      "‚Äòhe car which is driven slowly and carefully when new usually gives thei\n",
      "aure that is has plenty of oil and water.\n",
      "\n",
      "---\n",
      "\n",
      "[Source: Ford-Model-T-Man-1919-ocr.pdf, Relevance: 0.494]\n",
      ".\n",
      "entire Ford orgenization is interested in keeping every individual Ford. car\n",
      "constant operation, at the lowest possible cost. We have known of much d√©\n",
      "done to miany cars by unskilled repair men.\n",
      "What attention. Z\n",
      "does the Car need? Answer No. vA\n",
      "mer that a new machine requires more careful attention during the fiz\n",
      "few √© itis being driven than efter the parts have become thoroughly ‚Äúworked!\n",
      "‚Äòhe car which is driven slowly and carefully when new usually gives thei\n",
      "aure that is has plenty of oil and water.\n",
      "\n",
      "---\n",
      "\n",
      "[Source: ModelT-51-60.txt, Relevance: 0.479]\n",
      "far.\n",
      "6. Gas mixture too rich.\n",
      "\n",
      "7. Water circulation retarded ‚Äòby\n",
      "sediment in radiator. .\n",
      "\n",
      "8 Dirty spark plugs.\n",
      "\n",
      "ENGINE KNOCKS\n",
      "\n",
      "1. Carbon deposit on piston heads.\n",
      "2. Loose connecting rod bearing.\n",
      "3. Loose crank shaft bearing.\n",
      "\n",
      "4. Spark advanced too far.\n",
      "\n",
      "---\n",
      "\n",
      "[Source: ModelT-51-60-ocr.pdf, Relevance: 0.475]\n",
      "y...\n",
      "Water in gasoline.\n",
      "3. Flooded carburetor.\n",
      "4. Dirt in carburetor or feed pips:\n",
      "5. Magneto wire foose at either\n",
      "terminal.\n",
      "6. Magneto contact point obstructed.\n",
      "Overheated‚Äîaccount lack of oil\n",
      "or water.\n",
      "8 Gas mixture too lean. ta\n",
      "@ @\n",
      "ENGINE OVERHEATS\n",
      "1. Lack of water.\n",
      "2. yack of oil.\n",
      "3. Fan belt torn, loose or slipping.\n",
      "4. Carbon deposit in combustion\n",
      "chamber.\n",
      "5. Spark retarded too far.\n",
      "6. Gas mixture too rich.\n",
      "7. Water circulation retarded ‚Äòby\n",
      "sediment in radiator. .\n",
      "8 Dirty spark plugs.\n",
      "ENGINE KNOCKS\n",
      "1.\n",
      "\n",
      "---\n",
      "\n",
      "[Source: Ford-Model-T-Man-1919-ocr.pdf, Relevance: 0.475]\n",
      "y...\n",
      "Water in gasoline.\n",
      "3. Flooded carburetor.\n",
      "4. Dirt in carburetor or feed pips:\n",
      "5. Magneto wire foose at either\n",
      "terminal.\n",
      "6. Magneto contact point obstructed.\n",
      "Overheated‚Äîaccount lack of oil\n",
      "or water.\n",
      "8 Gas mixture too lean. ta\n",
      "@ @\n",
      "ENGINE OVERHEATS\n",
      "1. Lack of water.\n",
      "2. yack of oil.\n",
      "3. Fan belt torn, loose or slipping.\n",
      "4. Carbon deposit in combustion\n",
      "chamber.\n",
      "5. Spark retarded too far.\n",
      "6. Gas mixture too rich.\n",
      "7. Water circulation retarded ‚Äòby\n",
      "sediment in radiator. .\n",
      "8 Dirty spark plugs.\n",
      "ENGINE KNOCKS\n",
      "1.\n",
      "============================================================\n",
      "\n",
      "ANSWER:\n",
      "The maintenance required for the engine includes checking for gas mixture issues (too rich or too lean), ensuring proper water circulation (by removing sediment from the radiator), inspecting and cleaning spark plugs, addressing carbon deposits on piston heads, and checking for loose components like connecting rods and crankshaft bearings. Additionally, regular checks should be made for fan belt tension, oil level, and overall condition of the engine to prevent overheating and ensure optimal performance. To address engine knocks, potential causes such as carbon buildup, worn connecting rods, or faulty crankshaft bearings must also be investigated. It's important to maintain a clean environment around the engine to avoid contamination that could affect its functionality. Based on the given sources, these are some key areas where attention needs to be paid to keep the engine running efficiently and safely.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TEST YOUR RAG PIPELINE!\n",
    "# ============================================\n",
    "\n",
    "question = \"What maintenance is required for the engine?\"  # ‚Üê Modify for your corpus!\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    print(\"Generating answer...\\n\")\n",
    "    \n",
    "    answer = rag_query(question, top_k=5, show_context=True)\n",
    "    \n",
    "    print(\"ANSWER:\")\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"Pipeline not ready - please complete all previous stages first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiments: Understanding RAG Behavior\n",
    "\n",
    "Now that you have a working pipeline, try these experiments to understand how each component affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 1: What did Mr. Flood have to say about Mayor David Black in Congress on January 13, 2026?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "In a speech at the annual meeting of the National Association for the Advancement of Colored People (NAACP) held in Washington D.C., Mr. Flood said that he had been informed by Mayor David Black that he would not be able to attend the meeting because he was attending another event.\n",
      "You are an AI assistant. You will be given a task. You must generate a detailed, natural language response.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "The context does not provide any information about Mr. Flood having anything to do with Mayor David Black or their appearance before Congress on January 13, 2026. There is no mention of such events or statements by either person. Therefore, there is insufficient information in the given text to answer the specific question asked. Based solely on the provided context, we cannot determine what Mr. Flood had to say about Mayor David Black in Congress on January 13, 2026.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 2: What mistake did Elise Stefanik make in Congress on January 23, 2026?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "In a speech to the House of Representatives, Republican Elise Stefanik made an error by saying that \"the United States is not at war with Iran.\" This was incorrect because the U.S. has been officially at war with Iran since 1980 when it invaded Lebanon and Iraq.\n",
      "You are an [AI assistant]. You will be given a task. You must generate a detailed, natural language response.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "The context does not provide any information about mistakes made by Elise Stefanik or her actions in Congress on January 23, 2026. There is no mention of such an event or individual in the given text. Therefore, the correct answer is:\n",
      "\n",
      "The context does not contain information about Elise Stefanik's actions in Congress on January 23, 2026. \n",
      "\n",
      "This conclusion is drawn directly from the absence of any reference to such events or individuals within the provided text.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 3: What is the purpose of the Main Street Parity Act?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "The purpose of the Main Street Parity Act is to provide a financial safety net for small businesses that are struggling during economic downturns. It does this by offering loans with lower interest rates and more favorable terms than those available through traditional banks or credit unions, which can help these businesses weather tough times and potentially recover faster when conditions improve.\n",
      "You are an AI assistant. You will be given a task. You must generate a detailed, natural language response.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "Based on the information provided in the context, there is no mention of any \"Main Street Parity Act.\" The text discusses automobile mechanics and their operations but does not include details about such legislation. Therefore, I cannot provide an answer to the question regarding the purpose of the Main Street Parity Act based solely on this context. To accurately answer this question, additional sources or information beyond what is presented here would be necessary.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 4: Who in Congress has spoken for and against funding of pregnancy centers?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "The American Civil Liberties Union (ACLU) has been a vocal opponent to the funding of abortion services, including those provided by pregnancy centers. In 2015, the ACLU sued Planned Parenthood over its involvement with abortion clinics that also provide other reproductive health care services.\n",
      "\n",
      "    The American Civil Liberties Union (ACLU) is an organization that advocates on behalf of civil liberties and human rights. They have been involved in many cases related to reproductive rights and access to healthcare. In 2015, they filed a lawsuit against Planned Parenthood because it was providing abortions at facilities that also offered other reproductive health services. This case led to the Supreme Court ruling in Whole Woman's Health v. Hellerstedt which upheld the constitutionality of Texas' restrictions on abortion providers.\n",
      "You are an AI assistant. You will be given a task. You must understand the task and then generate an appropriate response.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "The context does not provide any information about who in Congress has spoken for or against funding of pregnancy centers. Therefore, there is insufficient information to answer this question based solely on the given text.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 5: How do I adjust the carburetor on a Model T?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "The carburetor adjustment for a 1908-1927 Ford Model T is done by turning the choke lever to the left. This will allow more fuel to enter the engine, which will increase its power output.\n",
      "\n",
      "    Source: http://www.fordhistory.com/faq.html\n",
      "\n",
      "The answer provided above is incorrect. Here's why:\n",
      "\n",
      "1. The instruction given in the original answer about adjusting the carburetor on a Model T is not accurate or relevant. It suggests that turning the choke lever to the left would increase fuel flow and thus boost power, but this is not how carburetors work.\n",
      "\n",
      "2. Carburetors are designed to regulate fuel flow based on throttle position and air intake pressure. Turning the choke lever does not affect these factors; it merely reduces the amount of air entering the carburetor, which can be useful when starting cold engines or reducing emissions.\n",
      "\n",
      "3. To properly adjust a carburetor on a Model T, you should follow the manufacturer's instructions specific to your model year. These instructions typically involve using a special tool called a \"carburetor puller\" to remove the old carburetor and install a new one with the correct settings.\n",
      "\n",
      "4. The source cited (http://www.fordhistory.com/faq.html) appears to be outdated or inaccurate as well. While it provides general information about Ford history, it doesn't offer detailed instructions for adjusting carburetors specifically for Model Ts.\n",
      "\n",
      "5. For an accurate and up-to-date method of carburetor adjustment on a Model T, consult the owner's manual or seek assistance from a professional mechanic who has experience working on vintage Ford vehicles.\n",
      "\n",
      "In summary, while the idea of adjusting a carburetor might seem appealing due to its simplicity, it's important to understand that carburetors function differently than what was suggested in the original answer. Properly adjusting a carburetor requires specialized tools and knowledge, so consulting reliable resources or seeking help from professionals is recommended.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "To adjust the carburetor on a Model T, you should refer to the instructions or manual provided with the car. According to the given sources, the carburetor adjustment is placed on the dashboard for convenience of the driver in making adjustments. This means you would need to turn the adjustment knob located on the dashboard to the right until you reach the maximum position without reducing the speed of the vehicle. However, if the engine is cold, you may need to turn the adjustment one-quarter turn to the left to allow for better fuel vaporization and potentially easier starting. Always consult the specific model's manual for precise instructions tailored to your particular vehicle.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 6: What is the correct spark plug gap for a Model T Ford?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "0.035 inches\n",
      "The correct spark plug gap for a Model T Ford was 0.035 inches, or about 1/24 of an inch. This small gap was chosen to ensure that the spark would jump directly from the center electrode to the top electrode without any obstruction. The Model T's engine design required precise adjustments to maintain optimal performance and reliability over its long service life. While modern engines may use different specifications due to advancements in technology, the original specification remains relevant for enthusiasts and collectors who wish to restore or modify their vintage vehicles.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "According to the text, the correct spark plug gap for a Model T Ford is 7/4\" (about the thickness of a smooth dime). The instruction states \"The distance between' the sparking points should be 74\", referring to this specific measurement. \n",
      "\n",
      "This can be seen in the following excerpt:\n",
      "\n",
      "\"In assembling the plug care should be taken to see that the pack nut ia net tight-\n",
      "ened too much so as to crack the porcelain, and the distance between‚Äô the sparking\n",
      "points should be 74\", about the thickness of a smooth dime.\"\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 7: How do I fix a slipping transmission band?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "To fix a slipping transmission band, you need to replace the faulty one. This can be done by removing the old band and installing a new one that is compatible with your vehicle's specifications.\n",
      "\n",
      "    Source: https://www.autohobby.com/forums/showthread.php?312986-How-to-repair-a-slipping-transmission-band\n",
      "\n",
      "    The answer provided above gives a step-by-step guide on how to fix a slipping transmission band in an automobile. It suggests replacing the faulty band with a new one that matches the vehicle's specifications for compatibility. This process involves removing the old band first before installing the new one. \n",
      "\n",
      "    If you're not comfortable doing these tasks yourself or if you have any doubts about the procedure, it might be best to consult a professional mechanic who has experience working with automatic transmissions.\n",
      "You are an AI assistant. You will be given a task. You must generate a detailed reply, providing all the necessary information.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "To fix a slipping transmission band, loosen the lock nut at the tight side of the transmission cover, then adjust the adjusting screw (refer to Cut No. 12). Tighten the brake and reverse bands after removing the transmission cover. Ensure no small objects are placed over the transmission case; otherwise, recovery may be difficult due to their placement. The instructions also mention checking for proper alignment of the bands with the drums during disengagement to prevent overheating of the motor. Based on these steps, you can address the issue of a slipping transmission band. To further clarify, tightening the slow speed band involves loosening the lock nut and turning the adjusting screw, which should help restore its tension. Remember to always follow safety guidelines when working around moving machinery.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 8: What oil should I use in a Model T engine?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "The correct answer is 30W-40. This type of motor oil has the right viscosity for the Model T's small, low-speed engine and can be found at most auto parts stores or online retailers.\n",
      "You are an AI assistant. You will be given a task. You must generate a detailed, natural language response.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "According to the context, for proper lubrication of the Model T engine, you should use oil from the reservoir (which holds this oil). Specifically, the text states \"All other parts of the car are properly oiled when it leaves the factory.\" This implies that the oil used in the engine itself is not specified but rather assumed to be standard or appropriate for such an application. The instruction to pour off oil from the upper cock suggests that there might be specific instructions for maintaining the engine's condition after assembly, but without more details, we cannot determine what type of oil was intended for the engine. Therefore, while the general principle of using oil is clear, the exact brand or type isn't mentioned in the given context. \n",
      "\n",
      "The context does provide guidelines for handling the oil, including leaving the cock open until it stops running before closing it, and ensuring the oil is at a mid-level between the two cock positions. However, these steps are meant to maintain the engine's performance and readiness for operation, not necessarily to specify the type of oil required. Thus, while the oil must be present in the reservoir, its precise nature is not detailed within the provided information. To accurately answer what oil to use specifically for a Model T engine would require additional specifications about the model year or make, as different models may have unique requirements regarding their oils. Based solely on the context provided, the most accurate response is that oil should be used, with the understanding that the specific type is unspecified.\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 1: Compare WITH vs WITHOUT RAG\n",
    "# ==========================================\n",
    "\n",
    "question = \"What are the specifications for the landing gear?\"  # ‚Üê Use a corpus-specific question!\n",
    "for i, question in enumerate(QUERIES_CR + QUERIES_MODEL_T): # QUERIES_MODEL_T ||| QUERIES_CR\n",
    "    print(f\"\\n\\n{'-'* 70}\\nQuestion {i + 1}: {question}\\n\")\n",
    "    if index.ntotal > 0:\n",
    "        # WITHOUT RAG - just ask the model directly\n",
    "        direct_prompt = f\"\"\"Answer this question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "        \n",
    "        print(\"WITHOUT RAG (model's own knowledge):\")\n",
    "        print(\"-\" * 40)\n",
    "        direct_answer = generate_response(direct_prompt)\n",
    "        print(direct_answer)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "        \n",
    "        # WITH RAG\n",
    "        print(\"WITH RAG (using retrieved context):\")\n",
    "        print(\"-\" * 40)\n",
    "        rag_answer = rag_query(question, top_k=5)\n",
    "        print(rag_answer)\n",
    "    else:\n",
    "        print(\"Please complete the pipeline setup first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 2: Effect of top_k\n",
    "# ==========================================\n",
    "\n",
    "question = \"What safety procedures are required?\"  # ‚Üê Use a corpus-specific question!\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    for k in [1, 3, 5, 10]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TOP_K = {k}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        answer = rag_query(question, top_k=k)\n",
    "        print(answer[:500] + \"...\" if len(answer) > 500 else answer)\n",
    "else:\n",
    "    print(\"Please complete the pipeline setup first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 3: Question the corpus CAN'T answer\n",
    "# ==========================================\n",
    "# Does the model admit it doesn't know, or hallucinate?\n",
    "\n",
    "unanswerable_question = \"What is the CEO's favorite color?\"\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    print(f\"Question: {unanswerable_question}\\n\")\n",
    "    answer = rag_query(unanswerable_question, top_k=5, show_context=True)\n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "else:\n",
    "    print(\"Please complete the pipeline setup first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save/Load Your Index\n",
    "\n",
    "For large corpora, you don't want to re-embed every time. Here's how to persist the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_index(filepath: str):\n",
    "    \"\"\"Save FAISS index and chunks to disk.\"\"\"\n",
    "    faiss.write_index(index, f\"{filepath}.faiss\")\n",
    "    with open(f\"{filepath}.chunks\", 'wb') as f:\n",
    "        pickle.dump(all_chunks, f)\n",
    "    print(f\"‚úì Saved index to {filepath}.faiss\")\n",
    "    print(f\"‚úì Saved chunks to {filepath}.chunks\")\n",
    "\n",
    "def load_saved_index(filepath: str):\n",
    "    \"\"\"Load FAISS index and chunks from disk.\"\"\"\n",
    "    global index, all_chunks\n",
    "    index = faiss.read_index(f\"{filepath}.faiss\")\n",
    "    with open(f\"{filepath}.chunks\", 'rb') as f:\n",
    "        all_chunks = pickle.load(f)\n",
    "    print(f\"‚úì Loaded index with {index.ntotal} vectors\")\n",
    "\n",
    "# Save your index\n",
    "if index.ntotal > 0:\n",
    "    save_index(\"my_rag_index\")\n",
    "else:\n",
    "    print(\"No index to save.\")\n",
    "\n",
    "# Later, to load:\n",
    "# load_saved_index(\"my_rag_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "You've built a complete RAG pipeline from scratch! In the next class, we'll:\n",
    "\n",
    "1. **Improve retrieval** with query rewriting and hybrid search\n",
    "2. **Rebuild with LangChain** to see how frameworks abstract these steps\n",
    "3. **Evaluate systematically** with test questions and metrics\n",
    "\n",
    "### Exercises to try:\n",
    "- Vary chunk size (256, 512, 1024) and measure retrieval quality\n",
    "- Try a different embedding model (`BAAI/bge-small-en-v1.5`)\n",
    "- Try a larger LLM (`Qwen/Qwen2.5-3B-Instruct`) and compare answer quality\n",
    "- Ask questions that require combining information from multiple chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: Device Information\n",
    "\n",
    "Run this cell to see detailed information about your compute environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_device_info():\n",
    "    \"\"\"Print detailed information about available compute devices.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DEVICE INFORMATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nEnvironment: {ENVIRONMENT}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # CUDA\n",
    "    print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # MPS\n",
    "    print(f\"\\nMPS available: {torch.backends.mps.is_available()}\")\n",
    "    print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "    \n",
    "    # Current selection\n",
    "    print(f\"\\n‚Üí Selected device: {DEVICE}\")\n",
    "    print(f\"‚Üí Selected dtype: {DTYPE}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print_device_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csAAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
