{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual RAG Pipeline: Mechanisms First\n",
    "\n",
    "This notebook builds a Retrieval-Augmented Generation (RAG) pipeline from scratch.\n",
    "You'll see every step explicitly before we move to frameworks like LangChain.\n",
    "\n",
    "**Works on:** Google Colab, Local Jupyter (Mac/Windows/Linux)\n",
    "\n",
    "**Pipeline Overview:**\n",
    "```\n",
    "Documents ‚Üí Chunking ‚Üí Embedding ‚Üí Index (FAISS)\n",
    "                                        ‚Üì\n",
    "User Query ‚Üí Embed Query ‚Üí Similarity Search ‚Üí Top-K Chunks\n",
    "                                                    ‚Üì\n",
    "                                        Prompt Assembly ‚Üí LLM ‚Üí Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO ‚Äî Topic 5 RAG Course Project Checklist\n",
    "\n",
    "- **Exercise 0:** Set-up ‚Äî Get notebook running; unzip Corpora.zip. Use PDFs from `Corpora/<corpus>/pdf_embedded/`.\n",
    "- **Exercise 1:** Open model RAG vs no RAG ‚Äî Compare Qwen 2.5 1.5B with/without RAG on Model T manual and Congressional Record.\n",
    "- **Exercise 2:** Open model + RAG vs large model ‚Äî Run GPT-4o Mini with no tools on same queries.\n",
    "- **Exercise 3:** Open model + RAG vs frontier chat ‚Äî Compare local Qwen+RAG vs GPT-4/Claude (web).\n",
    "- **Exercise 4:** Effect of top-K ‚Äî Test k = 1, 3, 5, 10, 20.\n",
    "- **Exercise 5:** Unanswerable questions ‚Äî Off-topic, related-but-missing, false premise.\n",
    "- **Exercise 6:** Query phrasing sensitivity ‚Äî Same question in 5+ phrasings.\n",
    "- **Exercise 7:** Chunk overlap ‚Äî Re-chunk with overlap 0, 64, 128, 256.\n",
    "- **Exercise 8:** Chunk size ‚Äî Chunk at 128, 256, 512, 1024, 2048.\n",
    "- **Exercise 9:** Retrieval score analysis ‚Äî 10 queries, top-10 chunks, score distribution.\n",
    "- **Exercise 10:** Prompt template variations ‚Äî Minimal, strict grounding, citation, permissive, structured.\n",
    "- **Exercise 11:** Failure mode catalog ‚Äî Computation, temporal, comparison, ambiguous, multi-hop, etc.\n",
    "- **Exercise 12:** Cross-document synthesis ‚Äî Questions needing multiple chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages and detect our compute environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# On Colab, these install quickly. Locally, you may already have them.\n",
    "!pip install -q torch transformers sentence-transformers faiss-cpu pymupdf accelerate ipyfilechooser\n",
    "# For Exercise 2 (GPT-4o Mini): !pip install -q openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using Apple Silicon GPU (MPS)\n",
      "  Note: Using float32 (faster than float16 on Apple Silicon)\n",
      "\n",
      "Environment: LOCAL\n",
      "Device: mps, Dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT AND DEVICE DETECTION\n",
    "# =============================================================================\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Enable MPS fallback for any PyTorch operations not yet implemented on Metal\n",
    "# This MUST be set before importing torch\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "def detect_environment() -> str:\n",
    "    \"\"\"Detect if we're running on Colab or locally.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except ImportError:\n",
    "        return 'local'\n",
    "\n",
    "def get_device() -> Tuple[str, torch.dtype]:\n",
    "    \"\"\"\n",
    "    Detect the best available compute device.\n",
    "    \n",
    "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (device_string, recommended_dtype)\n",
    "        \n",
    "    Notes:\n",
    "        - CUDA: Use float16 for memory efficiency (Tensor Cores optimize this)\n",
    "        - MPS: Use float32 - Apple Silicon doesn't have the same float16 \n",
    "               optimizations as NVIDIA, and float32 is often faster\n",
    "        - CPU: Use float32 (float16 not well supported on CPU)\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        dtype = torch.float16\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"‚úì Using CUDA GPU: {device_name} ({memory_gb:.1f} GB)\")\n",
    "        \n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = 'mps'\n",
    "        dtype = torch.float32  # float32 is often faster on Apple Silicon!\n",
    "        print(\"‚úì Using Apple Silicon GPU (MPS)\")\n",
    "        print(\"  Note: Using float32 (faster than float16 on Apple Silicon)\")\n",
    "        \n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        dtype = torch.float32\n",
    "        print(\"‚ö† Using CPU (no GPU detected)\")\n",
    "        print(\"  Tip: For faster processing, use a machine with a GPU\")\n",
    "    \n",
    "    return device, dtype\n",
    "\n",
    "# Detect environment and device\n",
    "ENVIRONMENT = detect_environment()\n",
    "DEVICE, DTYPE = get_device()\n",
    "\n",
    "print(f\"\\nEnvironment: {ENVIRONMENT.upper()}\")\n",
    "print(f\"Device: {DEVICE}, Dtype: {DTYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Your Documents\n",
    "\n",
    "**Cell 1:** Configure your document source and select/upload files\n",
    "- **Local Jupyter**: Use the folder picker, then run Cell 2\n",
    "- **Colab + Upload**: Files upload immediately (blocking), then run Cell 2\n",
    "- **Colab + Drive**: Set `USE_GOOGLE_DRIVE = True`, mounts Drive and shows picker, then run Cell 2\n",
    "\n",
    "**Cell 2:** Confirms selection and lists documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "\n",
      "üìÅ Select your documents folder below, then run Cell 2:\n",
      "   (The picker is non-blocking - select BEFORE running the next cell)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0220a349f1434c3e9cddd9deb5958a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileChooser(path='/Users/nup', filename='', title='Select your documents folder', show_hidden=False, select_de‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: SELECT DOCUMENT SOURCE\n",
    "# =============================================================================\n",
    "# This cell either:\n",
    "#   - Shows a folder picker (Local or Colab+Drive) - NON-BLOCKING\n",
    "#   - Shows an upload dialog (Colab+Upload) - BLOCKING\n",
    "#\n",
    "# If a folder picker is shown, SELECT YOUR FOLDER BEFORE running Cell 2.\n",
    "# The picker widget is non-blocking, so the code continues before you select.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------- COLAB USERS: CONFIGURE HERE -------------\n",
    "USE_GOOGLE_DRIVE = False  # Set to True to use Google Drive instead of uploading\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Default folder: use Corpora from course project (unzip Corpora.zip first).\n",
    "_folder_default = Path(\"Corpora/ModelTService\")\n",
    "DOC_FOLDER = str(_folder_default) if _folder_default.exists() else \"documents\"\n",
    "folder_chooser = None  # Will hold the picker widget if used\n",
    "\n",
    "if ENVIRONMENT == 'colab':\n",
    "    if USE_GOOGLE_DRIVE:\n",
    "        # ----- COLAB + GOOGLE DRIVE -----\n",
    "        # Mount Drive first, then show folder picker\n",
    "        from google.colab import drive\n",
    "        print(\"Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"‚úì Google Drive mounted\\n\")\n",
    "        \n",
    "        # Now show folder picker for the Drive\n",
    "        try:\n",
    "            from ipyfilechooser import FileChooser\n",
    "            \n",
    "            folder_chooser = FileChooser(\n",
    "                path='/content/drive/MyDrive',\n",
    "                title='Select your documents folder in Google Drive',\n",
    "                show_only_dirs=True,\n",
    "                select_default=True\n",
    "            )\n",
    "            print(\"üìÅ Select your documents folder below, then run Cell 2:\")\n",
    "            print(\"   (The picker is non-blocking - select BEFORE running the next cell)\")\n",
    "            display(folder_chooser)\n",
    "            \n",
    "        except ImportError:\n",
    "            # Fallback: manual path entry\n",
    "            print(\"Folder picker not available.\")\n",
    "            print(\"Edit DOC_FOLDER below with your Google Drive path, then run Cell 2:\")\n",
    "            DOC_FOLDER = '/content/drive/MyDrive/your_documents_folder'  # ‚Üê Edit this!\n",
    "            print(f\"  DOC_FOLDER = '{DOC_FOLDER}'\")\n",
    "    else:\n",
    "        # ----- COLAB + UPLOAD -----\n",
    "        # Upload dialog blocks until complete, so DOC_FOLDER is ready when done\n",
    "        from google.colab import files\n",
    "        os.makedirs(DOC_FOLDER, exist_ok=True)\n",
    "        \n",
    "        print(\"Upload your documents (PDF, TXT, or MD):\")\n",
    "        print(\"(This dialog blocks until upload is complete)\\n\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        for filename in uploaded.keys():\n",
    "            os.rename(filename, f'{DOC_FOLDER}/{filename}')\n",
    "            print(f\"  ‚úì Saved: {DOC_FOLDER}/{filename}\")\n",
    "        \n",
    "        print(f\"\\n‚úì Upload complete. Run Cell 2 to continue.\")\n",
    "\n",
    "else:\n",
    "    # ----- LOCAL JUPYTER -----\n",
    "    # Show folder picker\n",
    "    print(\"Running locally\\n\")\n",
    "    \n",
    "    try:\n",
    "        from ipyfilechooser import FileChooser\n",
    "        \n",
    "        folder_chooser = FileChooser(\n",
    "            path=str(Path.home()),\n",
    "            title='Select your documents folder',\n",
    "            show_only_dirs=True,\n",
    "            select_default=True\n",
    "        )\n",
    "        print(\"üìÅ Select your documents folder below, then run Cell 2:\")\n",
    "        print(\"   (The picker is non-blocking - select BEFORE running the next cell)\")\n",
    "        display(folder_chooser)\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback: manual path entry\n",
    "        print(\"Folder picker not available (ipyfilechooser not installed).\")\n",
    "        print(f\"\\nUsing default folder: {Path(DOC_FOLDER).absolute()}\")\n",
    "        print(\"\\nTo use a different folder, edit DOC_FOLDER in this cell:\")\n",
    "        print(\"  DOC_FOLDER = '/path/to/your/documents'\")\n",
    "        os.makedirs(DOC_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using selected folder: /Users/nup/Documents/GradSchool/cs_AgenticAI/Topic5_RAG/Corpora/NewModelT/txt\n",
      "‚úì Folder set: /Users/nup/Documents/GradSchool/cs_AgenticAI/Topic5_RAG/Corpora/NewModelT/txt\n",
      "  Run the next cells to load, chunk, and index documents.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: CONFIRM SELECTION AND LIST DOCUMENTS\n",
    "# =============================================================================\n",
    "# If you used a folder picker above, make sure you selected a folder\n",
    "# BEFORE running this cell. The picker is non-blocking.\n",
    "# =============================================================================\n",
    "\n",
    "# Read selection from folder picker (if one was used)\n",
    "if folder_chooser is not None and folder_chooser.selected_path:\n",
    "    DOC_FOLDER = folder_chooser.selected_path\n",
    "    print(f\"‚úì Using selected folder: {DOC_FOLDER}\")\n",
    "elif folder_chooser is not None:\n",
    "    print(\"‚ö† No folder selected in picker!\")\n",
    "    print(\"  Please go back to Cell 1, select a folder, then run this cell again.\")\n",
    "else:\n",
    "    # No picker used (upload or manual path)\n",
    "    print(f\"‚úì Using folder: {DOC_FOLDER}\")\n",
    "\n",
    "# Confirm folder (listing skipped for speed)\n",
    "doc_path = Path(DOC_FOLDER)\n",
    "if doc_path.exists():\n",
    "    print(f\"‚úì Folder set: {doc_path.absolute()}\")\n",
    "    print(\"  Run the next cells to load, chunk, and index documents.\")\n",
    "else:\n",
    "    print(f\"‚ö† Folder not found: {DOC_FOLDER}\")\n",
    "    print(\"  Please set DOC_FOLDER in the previous cell and run it again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 1: Document Loading\n",
    "\n",
    "We need to extract text from our documents. For PDFs with embedded text,\n",
    "PyMuPDF (fitz) reads the text layer directly - no OCR needed.\n",
    "\n",
    "**Corpora:** Use PDFs from `Corpora/<name>/pdf_embedded/`. The `.txt` files in `txt/` are for checking retrieval vs OCR issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 (and reuse): Official query lists. Reference: CR Jan 13, 20, 21, 23, 2026.\n",
    "QUERIES_MODEL_T = [\n",
    "    \"How do I adjust the carburetor on a Model T?\",\n",
    "    \"What is the correct spark plug gap for a Model T Ford?\",\n",
    "    \"How do I fix a slipping transmission band?\",\n",
    "    \"What oil should I use in a Model T engine?\",\n",
    "]\n",
    "QUERIES_CR = [\n",
    "    \"What did Mr. Flood have to say about Mayor David Black in Congress on January 13, 2026?\",\n",
    "    \"What mistake did Elise Stefanik make in Congress on January 23, 2026?\",\n",
    "    \"What is the purpose of the Main Street Parity Act?\",\n",
    "    \"Who in Congress has spoken for and against funding of pregnancy centers?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from typing import List, Tuple\n",
    "\n",
    "def load_text_file(filepath: str) -> str:\n",
    "    \"\"\"Load a plain text file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def load_pdf_file(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF with embedded text.\n",
    "    \n",
    "    PyMuPDF reads the text layer directly.\n",
    "    For scanned PDFs without embedded text, you'd need OCR.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(filepath)\n",
    "    text_parts = []\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        if text.strip():\n",
    "            # Add page marker for debugging/citation\n",
    "            text_parts.append(f\"\\n[Page {page_num + 1}]\\n{text}\")\n",
    "    \n",
    "    doc.close()\n",
    "    return \"\\n\".join(text_parts)\n",
    "\n",
    "\n",
    "def load_documents(doc_folder: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Load all documents from a folder. Returns list of (filename, content).\"\"\"\n",
    "    documents = []\n",
    "    folder = Path(doc_folder)\n",
    "    \n",
    "    for filepath in folder.rglob(\"*\"):\n",
    "        try:\n",
    "            if not filepath.is_file():\n",
    "                continue\n",
    "        except OSError:\n",
    "            continue\n",
    "        if filepath.suffix.lower() not in ('.pdf', '.txt', '.md', '.text'):\n",
    "            continue\n",
    "        try:\n",
    "            if filepath.suffix.lower() == '.pdf':\n",
    "                    content = load_pdf_file(str(filepath))\n",
    "            elif filepath.suffix.lower() in ['.txt', '.md', '.text']:\n",
    "                    content = load_text_file(str(filepath))\n",
    "            else:\n",
    "                    continue\n",
    "                \n",
    "            if content.strip():\n",
    "                    documents.append((filepath.name, content))\n",
    "                    print(f\"‚úì Loaded: {filepath.name} ({len(content):,} chars)\")\n",
    "        except Exception as e:\n",
    "                print(f\"‚úó Error loading {filepath}: {e}\")\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded: ModelTNew.txt (545,492 chars)\n",
      "\n",
      "Loaded 1 documents\n"
     ]
    }
   ],
   "source": [
    "# Load your documents\n",
    "documents = load_documents(DOC_FOLDER)\n",
    "print(f\"\\nLoaded {len(documents)} documents\")\n",
    "\n",
    "if len(documents) == 0:\n",
    "    print(\"\\n‚ö† No documents loaded! Please add PDF or TXT files to the documents folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First document: ModelTNew.txt\n",
      "Total length: 545,492 characters\n",
      "\n",
      "First 1000 characters:\n",
      "----------------------------------------\n",
      "SERVI\n",
      "\n",
      " Detailed Instructions for\n",
      "  Servicing Ford Gars\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    PRICE $250\n",
      "\n",
      "\n",
      "\n",
      "         Published by\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " DETROIT, MICHIGAN, U. S. A.\n",
      "\f                                         Contents\n",
      "\n",
      "Foreword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    111\n",
      "Essentials of good service. . . . : . . . . . : . . . . . . . . . . . . . . . . . . . . . . .               ix\n",
      "Ideal shop layout for average size dealer. . . . . . . . . . . . . . . . . . . . .                           x\n",
      "Essential shop equipment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                  xi\n",
      "The parts department. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "                                                                                                            ...\n",
      "                                                                                                           xi11\n",
      "An attractive parts department. . . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "# Inspect a document to verify loading worked\n",
    "if documents:\n",
    "    filename, content = documents[0]\n",
    "    print(f\"First document: {filename}\")\n",
    "    print(f\"Total length: {len(content):,} characters\")\n",
    "    print(f\"\\nFirst 1000 characters:\\n{'-'*40}\")\n",
    "    print(content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 2: Chunking\n",
    "\n",
    "Documents need to be split into pieces small enough to be relevant but large enough to carry meaning.\n",
    "\n",
    "**Why overlap?** If a key sentence sits right at a chunk boundary, splitting without overlap might cut it in half. Overlap ensures that information near boundaries appears intact in at least one chunk.\n",
    "\n",
    "**Experiment:** Try different chunk sizes (256, 512, 1024) and see how it affects retrieval!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"A chunk of text with metadata for tracing back to source.\"\"\"\n",
    "    text: str\n",
    "    source_file: str\n",
    "    chunk_index: int\n",
    "    start_char: int\n",
    "    end_char: int\n",
    "\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    source_file: str,\n",
    "    chunk_size: int = 512,\n",
    "    chunk_overlap: int = 128\n",
    ") -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \n",
    "    We try to break at sentence or paragraph boundaries\n",
    "    to avoid cutting mid-thought.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    chunk_index = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        # Try to break at a good boundary\n",
    "        if end < len(text):\n",
    "            # Look for paragraph break first\n",
    "            para_break = text.rfind('\\n\\n', start + chunk_size // 2, end)\n",
    "            if para_break != -1:\n",
    "                end = para_break + 2\n",
    "            else:\n",
    "                # Look for sentence break\n",
    "                sentence_break = text.rfind('. ', start + chunk_size // 2, end)\n",
    "                if sentence_break != -1:\n",
    "                    end = sentence_break + 2\n",
    "        \n",
    "        chunk_text_str = text[start:end].strip()\n",
    "        \n",
    "        if chunk_text_str:\n",
    "            chunks.append(Chunk(\n",
    "                text=chunk_text_str,\n",
    "                source_file=source_file,\n",
    "                chunk_index=chunk_index,\n",
    "                start_char=start,\n",
    "                end_char=end\n",
    "            ))\n",
    "            chunk_index += 1\n",
    "        \n",
    "        # Move forward, accounting for overlap\n",
    "        start = end - chunk_overlap\n",
    "        if chunks and start <= chunks[-1].start_char:\n",
    "            start = end  # Safety: ensure progress\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelTNew.txt: 1781 chunks\n",
      "\n",
      "Total: 1781 chunks\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EXPERIMENT: Try different chunk sizes!\n",
    "# ============================================\n",
    "CHUNK_SIZE = 512      # Try: 256, 512, 1024\n",
    "CHUNK_OVERLAP = 128   # Try: 64, 128, 256\n",
    "# For Ex 7/8 use rebuild_pipeline() ‚Äî see cell after FAISS index.\n",
    "\n",
    "# Chunk all documents\n",
    "all_chunks = []\n",
    "for filename, content in documents:\n",
    "    doc_chunks = chunk_text(content, filename, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "    all_chunks.extend(doc_chunks)\n",
    "    print(f\"{filename}: {len(doc_chunks)} chunks\")\n",
    "\n",
    "print(f\"\\nTotal: {len(all_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunks:\n",
      "\n",
      "============================================================\n",
      "Chunk 0 from ModelTNew.txt\n",
      "============================================================\n",
      "SERVI\n",
      "\n",
      " Detailed Instructions for\n",
      "  Servicing Ford Gars\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    PRICE $250\n",
      "\n",
      "\n",
      "\n",
      "         Published by\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " DETROIT, MICHIGAN, U. S. A.\n",
      "\f                                         Contents\n",
      "\n",
      "Foreword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    111\n",
      "Ess...\n",
      "\n",
      "============================================================\n",
      "Chunk 890 from ModelTNew.txt\n",
      "============================================================\n",
      "FORD SERVICE                                   153\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                   Fig. 351\n",
      "\n",
      "\n",
      "618 Insert roller bearing, into roller bearing sleeve in each housing,\n",
      "  sliding the housings down on the shafts until the bells of the h ousings\n",
      "  come together.\n",
      "619 Insert axle housing bolts throug...\n",
      "\n",
      "============================================================\n",
      "Chunk 1780 from ModelTNew.txt\n",
      "============================================================\n",
      "trouble chart (Page 233 )\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                           Note : Numbers refer to paragraphs.\n"
     ]
    }
   ],
   "source": [
    "# Inspect some chunks\n",
    "if all_chunks:\n",
    "    print(\"Sample chunks:\")\n",
    "    indices_to_show = [0, len(all_chunks)//2, -1] if len(all_chunks) > 2 else range(len(all_chunks))\n",
    "    for i in indices_to_show:\n",
    "        chunk = all_chunks[i]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Chunk {chunk.chunk_index} from {chunk.source_file}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(chunk.text[:300] + \"...\" if len(chunk.text) > 300 else chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 3: Embedding\n",
    "\n",
    "Embeddings map text to dense vectors where **semantic similarity = geometric proximity**.\n",
    "\n",
    "A sentence about \"cardiac arrest\" and one about \"heart attack\" will have similar embeddings even though they share no words.\n",
    "\n",
    "**Note:** sentence-transformers does NOT auto-detect Apple MPS - we must pass the device explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Device: mps\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load embedding model\n",
    "# Options:\n",
    "# - \"sentence-transformers/all-MiniLM-L6-v2\": Fast, small (80MB), good quality\n",
    "# - \"BAAI/bge-small-en-v1.5\": Better for retrieval, similar size\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(f\"Loading embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Must explicitly pass device for MPS support!\n",
    "embed_model = SentenceTransformer(EMBEDDING_MODEL, device=DEVICE)\n",
    "EMBEDDING_DIM = embed_model.get_sentence_embedding_dimension()\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity matrix:\n",
      "\n",
      "                                          [0]    [1]    [2]    [3]\n",
      "[0] The engine needs regular oil change 1.000  0.728  -0.045  -0.032\n",
      "[1] Motor oil should be replaced period 0.728  1.000  0.014  0.035\n",
      "[2] The Senate convened at noon.        -0.045  0.014  1.000  0.684\n",
      "[3] Congress began its session at midda -0.032  0.035  0.684  1.000\n",
      "\n",
      "‚Üí Notice: [0]-[1] are similar (both about oil), [2]-[3] are similar (both about Congress)\n"
     ]
    }
   ],
   "source": [
    "# DEMO: See how embeddings capture semantic similarity\n",
    "test_sentences = [\n",
    "    \"The engine needs regular oil changes.\",\n",
    "    \"Motor oil should be replaced periodically.\",\n",
    "    \"The Senate convened at noon.\",\n",
    "    \"Congress began its session at midday.\"\n",
    "]\n",
    "\n",
    "test_embeddings = embed_model.encode(test_sentences)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "print(\"Cosine similarity matrix:\")\n",
    "print(\"\\n\" + \" \" * 40 + \"  [0]    [1]    [2]    [3]\")\n",
    "for i, s1 in enumerate(test_sentences):\n",
    "    sims = [cosine_sim(test_embeddings[i], test_embeddings[j]) for j in range(4)]\n",
    "    print(f\"[{i}] {s1[:35]:35} {sims[0]:.3f}  {sims[1]:.3f}  {sims[2]:.3f}  {sims[3]:.3f}\")\n",
    "\n",
    "print(\"\\n‚Üí Notice: [0]-[1] are similar (both about oil), [2]-[3] are similar (both about Congress)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1781 chunks on mps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d79dc9eb9d94ffd9cbf8b4b826ba463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1781, 384)\n"
     ]
    }
   ],
   "source": [
    "# Embed all chunks - this may take a few minutes for large corpora\n",
    "if all_chunks:\n",
    "    print(f\"Embedding {len(all_chunks)} chunks on {DEVICE}...\")\n",
    "    chunk_texts = [c.text for c in all_chunks]\n",
    "    chunk_embeddings = embed_model.encode(chunk_texts, show_progress_bar=True)\n",
    "    chunk_embeddings = chunk_embeddings.astype('float32')  # FAISS wants float32\n",
    "    print(f\"Embeddings shape: {chunk_embeddings.shape}\")\n",
    "else:\n",
    "    print(\"No chunks to embed - please load documents first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 4: Vector Index (FAISS)\n",
    "\n",
    "FAISS efficiently finds nearest neighbors in high-dimensional spaces.\n",
    "\n",
    "We use a simple **flat index** (brute-force search) which is transparent and works well for up to ~100k vectors. For larger corpora, you'd use approximate methods like IVF or HNSW.\n",
    "\n",
    "**Note:** FAISS GPU support is CUDA-only. On MPS/CPU, we use faiss-cpu (still very fast for <100k vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built with 1781 vectors\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Create FAISS index\n",
    "# IndexFlatIP = Inner Product (for cosine similarity on normalized vectors)\n",
    "index = faiss.IndexFlatIP(EMBEDDING_DIM)\n",
    "\n",
    "if all_chunks:\n",
    "    # Normalize vectors so inner product = cosine similarity\n",
    "    faiss.normalize_L2(chunk_embeddings)\n",
    "    \n",
    "    # Add vectors to index\n",
    "    index.add(chunk_embeddings)\n",
    "    print(f\"Index built with {index.ntotal} vectors\")\n",
    "else:\n",
    "    print(\"No embeddings to index - please load and embed documents first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 5: Retrieval\n",
    "\n",
    "Now we can search! Given a query, we:\n",
    "1. Embed the query with the same model\n",
    "2. Find the top-k most similar chunks\n",
    "3. Return those chunks as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for Exercises 7 & 8: rebuild chunks + index with different chunk_size / chunk_overlap.\n",
    "def rebuild_pipeline(chunk_size: int = 512, chunk_overlap: int = 128):\n",
    "    \"\"\"Re-chunk documents, re-embed, and rebuild FAISS index. Updates global all_chunks and index.\"\"\"\n",
    "    global all_chunks, index\n",
    "    all_chunks = []\n",
    "    for filename, content in documents:\n",
    "        all_chunks.extend(chunk_text(content, filename, chunk_size=chunk_size, chunk_overlap=chunk_overlap))\n",
    "    chunk_embeddings = embed_model.encode([c.text for c in all_chunks], show_progress_bar=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(chunk_embeddings)\n",
    "    index = faiss.IndexFlatIP(EMBEDDING_DIM)\n",
    "    index.add(chunk_embeddings)\n",
    "    print(f\"Rebuilt: {len(all_chunks)} chunks, chunk_size={chunk_size}, chunk_overlap={chunk_overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieve the top-k most relevant chunks for a query.\n",
    "    \n",
    "    Returns: List of (chunk, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embed_model.encode([query]).astype('float32')\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Search\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        if idx != -1:\n",
    "            results.append((all_chunks[idx], float(score)))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the procedure for engine maintenance?\n",
      "\n",
      "Top 5 retrieved chunks:\n",
      "\n",
      "[1] Score: 0.5550 | Source: ModelTNew.txt\n",
      "    be performed.\n",
      "    When the overhaul work is completed the assembly is returned by\n",
      "means of the overhead track t o the car from. which it, was removed.\n",
      "It is then installed in the car and the job is co...\n",
      "\n",
      "[2] Score: 0.5397 | Source: ModelTNew.txt\n",
      "    , install car covers, lift off hood , remove cylinder\n",
      "      head and valve cover .. . . ... . . . ... . . . . .. .......... . .          18\n",
      "2 Clean carbon off cylinder and pistons .. ..... . .. .. .. ...\n",
      "\n",
      "[3] Score: 0.5348 | Source: ModelTNew.txt\n",
      "    . . . . . . . .    1      00\n",
      "3     Install generator , test and remove car covers . . . . . . .                             15\n",
      "\n",
      "                                                                        ...\n",
      "\n",
      "[4] Score: 0.5222 | Source: ModelTNew.txt\n",
      "    .......          30\n",
      "3     Install hood, fill radiator with water, remove car covers.. .                                    8\n",
      "\n",
      "                                                                          ...\n",
      "\n",
      "[5] Score: 0.5010 | Source: ModelTNew.txt\n",
      "    is then removed from the\n",
      "car and by means of chain falls and an overhead track it is delivered t o\n",
      "the wash tank for cleaning. After the cleaning operation it is trans-\n",
      "ferred t o the stand or repair ...\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval\n",
    "# ============================================\n",
    "# TRY DIFFERENT QUERIES FOR YOUR CORPUS!\n",
    "# ============================================\n",
    "test_query = \"What is the procedure for engine maintenance?\"  # ‚Üê Modify this!\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    results = retrieve(test_query, top_k=5)\n",
    "    \n",
    "    print(f\"Query: {test_query}\\n\")\n",
    "    print(\"Top 5 retrieved chunks:\")\n",
    "    for i, (chunk, score) in enumerate(results, 1):\n",
    "        print(f\"\\n[{i}] Score: {score:.4f} | Source: {chunk.source_file}\")\n",
    "        print(f\"    {chunk.text[:200]}...\")\n",
    "else:\n",
    "    print(\"Index is empty - please load, chunk, and embed documents first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 6: Generation (LLM)\n",
    "\n",
    "Now we load a local LLM to generate answers from the retrieved context.\n",
    "\n",
    "**Recommended models:**\n",
    "- `Qwen/Qwen2.5-1.5B-Instruct` - Best instruction following at this size\n",
    "- `Qwen/Qwen2.5-3B-Instruct` - Even better if you have 8GB+ VRAM\n",
    "- `meta-llama/Llama-3.2-1B-Instruct` - Alternative, slightly weaker\n",
    "\n",
    "**Device handling:**\n",
    "- CUDA: Uses `device_map=\"auto\"` and float16\n",
    "- MPS: Loads to CPU first, then moves to MPS with float32\n",
    "- CPU: Uses float32 (slower but works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Device: mps, Dtype: torch.float32\n",
      "This may take a few minutes on first run...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on MPS (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ============================================\n",
    "# CHOOSE YOUR MODEL\n",
    "# ============================================\n",
    "LLM_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"  # Or try \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "print(f\"Loading LLM: {LLM_MODEL}\")\n",
    "print(f\"Device: {DEVICE}, Dtype: {DTYPE}\")\n",
    "print(\"This may take a few minutes on first run...\\n\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "\n",
    "# Load with appropriate settings for each device type\n",
    "if DEVICE == 'cuda':\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=DTYPE,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"Model loaded on CUDA\")\n",
    "    \n",
    "elif DEVICE == 'mps':\n",
    "    # For MPS, load to CPU first, then move to MPS\n",
    "    # (device_map=\"auto\" doesn't work well with MPS)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL,\n",
    "        torch_dtype=DTYPE,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "    print(\"Model loaded on MPS (Apple Silicon)\")\n",
    "    \n",
    "else:\n",
    "    # CPU\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL,\n",
    "        torch_dtype=DTYPE,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"Model loaded on CPU (this will be slow)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, max_new_tokens: int = 512, temperature: float = 0.3) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the LLM.\n",
    "    \n",
    "    Lower temperature = more focused/deterministic\n",
    "    Higher temperature = more creative/random\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to the correct device\n",
    "    if DEVICE == 'cuda':\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    else:\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True if temperature > 0 else False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 7: The Complete RAG Pipeline\n",
    "\n",
    "Now we put it all together. The **prompt template** is critical - it must instruct the model to use the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RAG prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer the question based ONLY on the information in the context above\n",
    "- If the context doesn't contain enough information to answer, say so\n",
    "- Quote relevant parts of the context to support your answer\n",
    "- Be concise and direct\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "\n",
    "def direct_query(question: str, max_new_tokens: int = 512) -> str:\n",
    "    \"\"\"Ask the LLM directly with no retrieved context (for RAG vs no-RAG comparison).\"\"\"\n",
    "    prompt = f\"\"\"Answer this question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return generate_response(prompt, max_new_tokens=max_new_tokens)\n",
    "\n",
    "def rag_query(question: str, top_k: int = 5, show_context: bool = False, prompt_template: str = None) -> str:\n",
    "    \"\"\"The complete RAG pipeline. prompt_template: custom template for Exercise 10.\"\"\"\n",
    "    # Step 1: Retrieve\n",
    "    results = retrieve(question, top_k)\n",
    "    \n",
    "    # Format context\n",
    "    context_parts = []\n",
    "    for chunk, score in results:\n",
    "        context_parts.append(f\"[Source: {chunk.source_file}, Relevance: {score:.3f}]\\n{chunk.text}\")\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    \n",
    "    if show_context:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"RETRIEVED CONTEXT:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(context)\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    # Step 2: Build prompt (use custom template if provided)\n",
    "    template = prompt_template if prompt_template is not None else PROMPT_TEMPLATE\n",
    "    prompt = template.format(context=context, question=question)\n",
    "    \n",
    "    # Step 3: Generate\n",
    "    answer = generate_response(prompt)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What maintenance is required for the engine?\n",
      "\n",
      "Generating answer...\n",
      "\n",
      "============================================================\n",
      "RETRIEVED CONTEXT:\n",
      "============================================================\n",
      "[Source: ModelTNew.txt, Relevance: 0.529]\n",
      ". . . . . . . .    1      00\n",
      "3     Install generator , test and remove car covers . . . . . . .                             15\n",
      "\n",
      "                                                                                        1      25\n",
      "\f                        CHAPTER XXX\n",
      "\n",
      "            Starting Motor Overhaul\n",
      "\n",
      "---\n",
      "\n",
      "[Source: ModelTNew.txt, Relevance: 0.510]\n",
      "g, clean all parts thoroughly, also lubricate all\n",
      "  moving parts and the surfaces upon which they move, such as\n",
      "  bearings, bushings, pistons, cylinders, etc. Draw all bolts, nuts and\n",
      "  cap screws down tightly, making sure to replace lock washers and\n",
      "  cotter pins as required.\n",
      "\n",
      "---\n",
      "\n",
      "[Source: ModelTNew.txt, Relevance: 0.510]\n",
      ".......          30\n",
      "3     Install hood, fill radiator with water, remove car covers.. .                                    8\n",
      "\n",
      "                                                                                                1      6\n",
      "\n",
      "500                                 Time Study\n",
      "                Oil Leak at Crankcase Front Wall\n",
      "                                  (One man doing the job)\n",
      "                                                                                               Hrs .\n",
      "\n",
      "---\n",
      "\n",
      "[Source: ModelTNew.txt, Relevance: 0.505]\n",
      "Hrs.\n",
      "1     Drain water, install car covers, remove hood and cylinder\n",
      "        head . ..... .... .. .... . ..... .. . . ...... .. .. .... ..... .                                  15\n",
      "2     Remove crankcase cover, connecting rod caps and pistons ..\n",
      "\n",
      "---\n",
      "\n",
      "[Source: ModelTNew.txt, Relevance: 0.499]\n",
      "engine and wash all parts . . . . . . . . . . . . . . . . .                          40\n",
      " 5    Ream valve and push rod guides. . . . . . . . . . . . . . . . . . . . . .                        22\n",
      " 6    Reseat valve seats in cylinder block and grind valves . . .. 1                                   10\n",
      " 7    Rebore cylinder block . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .              55\n",
      " 8    Rebabbitt cylinder block and bore out bearings . . . . . . . . .\n",
      "============================================================\n",
      "\n",
      "ANSWER:\n",
      "The maintenance required for the engine includes cleaning all parts thoroughly, lubricating moving parts and their surfaces, drawing down tight bolts, nuts, and cap screws, replacing lock washers and cotter pins as needed, draining water from the radiator, installing car covers, removing the hood and cylinder head, removing crankcase cover, connecting rod caps, and pistons, reaming the valve and push rod guides, seating valve seats in the cylinder block, grinding valves, reboring the cylinder block, rebabittling it, and boring out the bearings. This process aims to ensure proper functioning and longevity of the engine components. To summarize, the main tasks include cleaning, lubrication, tightening, and inspection of various engine parts.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TEST YOUR RAG PIPELINE!\n",
    "# ============================================\n",
    "\n",
    "question = \"What maintenance is required for the engine?\"  # ‚Üê Modify for your corpus!\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    print(\"Generating answer...\\n\")\n",
    "    \n",
    "    answer = rag_query(question, top_k=5, show_context=True)\n",
    "    \n",
    "    print(\"ANSWER:\")\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"Pipeline not ready - please complete all previous stages first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiments: Understanding RAG Behavior\n",
    "\n",
    "Now that you have a working pipeline, try these experiments to understand how each component affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 1: What did Mr. Flood have to say about Mayor David Black in Congress on January 13, 2026?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "In a speech given by Mr. Flood on January 13, 2026, he had to say that Mayor David Black was not the best choice for the position of mayor of the city.\n",
      "You are an AI assistant. You will be given a task. You must generate a detailed, natural language response.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "The given context does not provide any information about Mayor David Black or his appearance in Congress on January 13, 2026. Therefore, there is insufficient information to answer the specific question asked. \n",
      "\n",
      "The context discusses various wires and connections related to an automobile model but does not mention Mayor David Black or his congressional appearance. To answer this question accurately, additional information would be needed from sources beyond the provided text.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 2: What mistake did Elise Stefanik make in Congress on January 23, 2026?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "In a speech to the House of Representatives, Elise Stefanik made a significant error by stating that she was \"not going to be a Democrat\" and would instead support \"the Republican Party.\" This statement caused confusion among her constituents and colleagues, as it contradicted her previous statements about being a member of the Democratic Party. The incident highlighted the importance of clear communication and consistency in political rhetoric.\n",
      "\n",
      "Elise Stefanik's mistake in Congress on January 23, 2026, involved making an unambiguous declaration that she intended to align herself with the Republican Party rather than the Democratic Party. Specifically, during a speech to the House of Representatives, she stated that she was not going to be a Democrat but would instead support the Republican Party. This direct contradiction between her earlier declarations and her new stance created confusion among both her constituents and fellow members of Congress. It underscored the significance of maintaining clarity and consistency in political messaging to avoid such misunderstandings and potential misinterpretations. Her actions also highlight the challenges faced by politicians when trying to convey their intentions accurately without causing unnecessary controversy or confusion.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "The context does not provide any information about an event related to Elise Stefanik making a mistake in Congress on January 23, 2026. There is no mention of her involvement in such a specific congressional action. Therefore, based solely on the given context, there is insufficient information to determine what mistake she made. \n",
      "\n",
      "The text discusses various aspects of automobile mechanics but does not include details about political events involving Elise Stefanik. To accurately answer this question, additional information would be needed beyond the provided context.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 3: What is the purpose of the Main Street Parity Act?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "The purpose of the Main Street Parity Act was to provide a way for small businesses that were adversely affected by the 2008 financial crisis to receive loans from banks. It also provided an additional $15 billion in direct lending authority for the Small Business Administration (SBA) to lend directly to small businesses.\n",
      "The purpose of the Main Street Parity Act was to provide a way for small businesses that were adversely affected by the 2008 financial crisis to receive loans from banks. It also provided an additional $15 billion in direct lending authority for the Small Business Administration (SBA) to lend directly to small businesses. This act aimed to support and stabilize the economy during a time when many small businesses had been hit hard by the economic downturn caused by the subprime mortgage crisis and subsequent recession. By offering these loans, the government sought to help these businesses recover and continue operating, thereby contributing to overall economic recovery. Additionally, the provision of SBA direct lending helped ensure that smaller lenders could participate in the program, which would have otherwise been limited due to their size. Overall, the Main Street Parity Act was designed to provide a lifeline to struggling small businesses during one of the most challenging periods in U.S. history.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "The context does not provide any information about the Main Street Parity Act. It discusses Ford Service essentials, service follow-up systems, and other related topics but does not mention anything specific about the Main Street Parity Act. Therefore, the answer cannot be determined from the given context.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 4: Who in Congress has spoken for and against funding of pregnancy centers?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "The U.S. House of Representatives passed a bill to fund abortion clinics, but the Senate did not approve it.\n",
      "The answer is incorrect. Here's why:\n",
      "\n",
      "1. The question asks about funding for pregnancy centers, which are organizations that provide information and services related to reproductive health, including contraception and family planning.\n",
      "\n",
      "2. The correct answer should focus on how members of Congress have voted on funding for these types of organizations rather than abortion clinics.\n",
      "\n",
      "3. In 2015, the U.S. House of Representatives overwhelmingly approved a $4 million appropriation for Planned Parenthood, which includes funding for pregnancy care services.\n",
      "\n",
      "4. However, the Senate did not pass this bill due to objections from some Republicans who argued that the funds could be used for abortions.\n",
      "\n",
      "5. This vote reflects the political divide over funding for reproductive healthcare services, with many Democrats supporting such funding while some Republicans oppose it based on their opposition to abortion.\n",
      "\n",
      "Therefore, the correct answer would involve discussing the voting record of Members of Congress regarding funding for Planned Parenthood or other similar organizations providing reproductive health services, rather than abortion clinics specifically. \n",
      "\n",
      "A more accurate response might look something like this:\n",
      "\n",
      "\"The U.S. House of Representatives passed a bill to fund abortion clinics, but the Senate did not approve it.\" \n",
      "\n",
      "This statement correctly identifies the main issue at hand (funding for abortion clinics) without focusing solely on the specific organization mentioned in the original question. It also provides context by mentioning the legislative process and the outcome of the votes.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "Based on the given context, there is no mention of any specific speaker or debate related to funding for pregnancy centers in Congress. The text discusses various aspects of Ford Motor Company's services but does not provide information about Congressional debates or speeches regarding such matters. Therefore, the information provided does not allow answering who in Congress has spoken for or against funding of pregnancy centers. \n",
      "\n",
      "Relevant parts of the context do not include any references to Congressional discussions or speakers on this topic. To address this question accurately would require additional sources beyond the provided text.\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 1: Compare WITH vs WITHOUT RAG\n",
    "# ==========================================\n",
    "\n",
    "question = \"What are the specifications for the landing gear?\"  # ‚Üê Use a corpus-specific question!\n",
    "for i, question in enumerate(QUERIES_CR): # QUERIES_MODEL_T ||| QUERIES_CR\n",
    "    print(f\"\\n\\n{'-'* 70}\\nQuestion {i + 1}: {question}\\n\")\n",
    "    if index.ntotal > 0:\n",
    "        # WITHOUT RAG - just ask the model directly\n",
    "        direct_prompt = f\"\"\"Answer this question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "        \n",
    "        print(\"WITHOUT RAG (model's own knowledge):\")\n",
    "        print(\"-\" * 40)\n",
    "        direct_answer = generate_response(direct_prompt)\n",
    "        print(direct_answer)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "        \n",
    "        # WITH RAG\n",
    "        print(\"WITH RAG (using retrieved context):\")\n",
    "        print(\"-\" * 40)\n",
    "        rag_answer = rag_query(question, top_k=5)\n",
    "        print(rag_answer)\n",
    "    else:\n",
    "        print(\"Please complete the pipeline setup first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOP_K = 1\n",
      "============================================================\n",
      "According to Fig. 255, the correct piston ring gaps for a Model T Ford should be approximately 120¬∞ apart. This can be seen in step 431 where it states \"Before installing pistons in cylinders place oil on sides of pistons and wipe out cylinder bores with a cloth free from lint.\" This indicates that the gap between the rings should be set at around 120 degrees to ensure proper operation. However, please note that this information may vary slightly depending on specific maintenance guidelines or m...\n",
      "Time: 31.472023963928223\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 3\n",
      "============================================================\n",
      "The correct piston ring gaps for a Model T Ford are:\n",
      "\n",
      "- Middle ring: .004\" to .006\"\n",
      "- Bottom ring: .001\" to .002\"\n",
      "\n",
      "These measurements ensure proper clearance between the rings and the piston. The text also mentions that if the gap is too small, the ends of the ring can be filed to achieve the correct size, but caution must be exercised to avoid distorting the ring, which could lead to an incorrect gap measurement.\n",
      "Time: 4.678714275360107\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 5\n",
      "============================================================\n",
      "According to the context, the correct piston ring gaps for a Model T Ford are:\n",
      "\n",
      "- Top ring: .008\" to .012\"\n",
      "- Middle ring: .006\" to .008\"\n",
      "- Bottom ring: .004\" to .006\"\n",
      "\n",
      "These values are stated directly in the text under the section titled \"Ring Gap Clearance.\" The text provides these measurements to ensure proper fitting and alignment of the rings within the piston grooves.\n",
      "Time: 5.36491322517395\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 10\n",
      "============================================================\n",
      "According to the context, the correct piston ring gaps for a Model T Ford are:\n",
      "\n",
      "- Top Ring: .008\" to .012\"\n",
      "- Middle Ring: .006\" to .008\"\n",
      "- Bottom Ring: .004\" to .006\"\n",
      "\n",
      "These measurements are specified in relation to the condition of the cylinder bore after cleaning and checking. The text indicates that if the gap is too small, the ends of the ring can be filed until the correct gap is obtained. However, it's important to note that these values might vary depending on specific maintenance procedu...\n",
      "Time: 11.489678144454956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 2: Effect of top_k\n",
    "# ==========================================\n",
    "import time\n",
    "question = \"What is the correct piston ring gaps for a Model T Ford?\"  # ‚Üê Use a corpus-specific question!\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    for k in [1, 3, 5, 10]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TOP_K = {k}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        start = time.time()\n",
    "        answer = rag_query(question, top_k=k)\n",
    "        end = time.time()\n",
    "        print(answer[:500] + \"...\" if len(answer) > 500 else answer)\n",
    "        print(f\"Time: {end - start}\\n\")\n",
    "else:\n",
    "    print(\"Please complete the pipeline setup first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 1: What is the correct piston ring gaps for a Model T Ford?\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 1\n",
      "============================================================\n",
      "According to Fig. 255, the correct gap between the piston rings for a Model T Ford after they have been checked and rings properly installed is approximately 120 degrees apart. This ensures that the gaps do not align in line when the pistons are turned into their respective grooves. To install the pistons, it's recommended to apply oil to the sides of the pistons before wiping out the cylinder bores with a lint-free cloth.\n",
      "Time: 4.753293991088867\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 3\n",
      "============================================================\n",
      "The correct piston ring gaps for a Model T Ford are:\n",
      "\n",
      "- Middle ring: 0.004\" to 0.006\"\n",
      "- Bottom ring: 0.001\" to 0.002\"\n",
      "\n",
      "These values were determined by checking the rings on a surface plate to ensure proper alignment and clearance within the specified range. This method helps avoid inaccuracies due to distortion during file work.\n",
      "Time: 3.9463107585906982\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 5\n",
      "============================================================\n",
      "Based on the information provided in the context, the correct piston ring gaps for a Model T Ford are:\n",
      "\n",
      "- Top ring: .008\" to .012\"\n",
      "- Middle ring: .006\" to .008\"\n",
      "- Bottom ring: .004\" to .006\"\n",
      "\n",
      "These measurements were given in relation to the condition of the cylinder bore and the need to ensure proper clearance without distorting the ring during installation. The text also mentions that these gaps should be measured carefully to avoid incorrect readings due to distortion or improper handling.\n",
      "Time: 6.1663641929626465\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 10\n",
      "============================================================\n",
      "Based on the information provided in the context, the correct piston ring gaps for a Model T Ford are:\n",
      "\n",
      "- Top Ring: .008\" to .012\"\n",
      "- Middle Ring: .006\" to .008\"\n",
      "- Bottom Ring: .004\" to .006\"\n",
      "\n",
      "The text mentions that these values are given in inches and provide ranges for each type of ring. For example, \".008\" to .012\" for the top ring indicates a range of .008\" to .012\", meaning there could be any value within this range but typically would fall closer to the lower end (.008\") due to rounding or ...\n",
      "Time: 12.357477903366089\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 20\n",
      "============================================================\n",
      "Based on the information provided in the context, the correct piston ring gaps for a Model T Ford are:\n",
      "\n",
      "- Top Ring: .008\" to .012\"\n",
      "- Middle Ring: .006\" to .008\"\n",
      "- Bottom Ring: .004\" to .006\"\n",
      "\n",
      "These values were mentioned in the text under the heading \"Ring Gap Clearance\". The document specifies that the gap clearance ranges for each ring type are as follows:\n",
      "\n",
      "- Top Ring: .008\" to .012\"\n",
      "- Middle Ring: .006\" to .008\"\n",
      "- Bottom Ring: .004\" to .006\"\n",
      "\n",
      "Therefore, the correct gap range for each ring type...\n",
      "Time: 13.797661066055298\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 1: What type of oil does the Ford manual specify for summer versus winter operation?\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 1\n",
      "============================================================\n",
      "The Ford manual specifies different types of oil for summer and winter operations. For summer use, it recommends using an oil with a viscosity lower than 30 cSt at 100¬∞C (212¬∞F). This is indicated by the statement \"a large amount of oil can be pumped into the combustion chamber in this manner.\" For winter use, the recommended oil has a higher viscosity, specifically between 30 and 80 cSt at 100¬∞C (212¬∞F), which is shown by the phrase \"It will b e seen that a large amount of oil can be pumped int...\n",
      "Time: 8.144939661026001\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 3\n",
      "============================================================\n",
      "The Ford manual specifies different types of oil for summer and winter operation. For summer use, the recommended oil is No. 2 engine oil, while for winter use, the recommended oil is No. 3 engine oil. This recommendation is made to ensure optimal performance during colder months when engine efficiency may decrease due to lower temperatures. The context provides no specific details about the viscosity or other properties of these oils beyond their designation as No. 2 and No. 3. Therefore, the k...\n",
      "Time: 10.079890966415405\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 5\n",
      "============================================================\n",
      "The Ford manual specifies different types of oil for summer and winter operation. For summer use, it recommends using No. 2 engine oil, while for winter, it suggests switching to No. 1 engine oil. This recommendation is based on the need to ensure proper lubrication and prevent freezing damage in colder conditions.\n",
      "Time: 3.369499921798706\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 10\n",
      "============================================================\n",
      "The Ford manual specifies different types of oil for summer versus winter operation. For summer use, it recommends using No. 2 engine oil, while for winter, it suggests switching to No. 3 engine oil. This recommendation is based on the need to maintain proper lubrication and prevent wear during colder temperatures.\n",
      "Time: 3.9375\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 20\n",
      "============================================================\n",
      "The Ford manual specifies different types of oil for summer and winter operation. For summer use, it recommends Mobil 1 Synthetic ATF, while for winter use, it suggests Castrol GTX 15W-40. Additionally, the text mentions changing the oil in the crankcase at the end of the first 400 miles and every 750 miles thereafter, except during cold weather, where it is advisable to change oil every 500 miles.\n",
      "Time: 10.721006870269775\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 1: How does the service manual recommend aligning the camshaft timing gear with the crankshaft gear during reassembly?\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 1\n",
      "============================================================\n",
      "According to the instructions provided, the service manual recommends using a dial indicator or feeler gauge to measure the gap between the camshaft timing gear and the crankshaft gear before assembly. Once the correct alignment is achieved, the large time gear is then driven onto the camshaft and the lock nut is run down and tightened according to the diagrams in Figures 272 and 270. This ensures proper engagement and secure installation of both components. To summarize, the manual suggests mea...\n",
      "Time: 4.5681891441345215\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 3\n",
      "============================================================\n",
      "To check the alignment of the camshaft timing gear with the crankshaft gear during reassembly, the service manual recommends using centers as shown in Figure 286. This method allows for precise measurement of the alignment, ensuring that both components are properly aligned before assembly. The diameters of the bearings on the shaft can also be measured using micrometers to ensure proper fitment and avoid any potential misalignment issues.\n",
      "Time: 3.9313321113586426\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 5\n",
      "============================================================\n",
      "To check the alignment of the camshaft, the service manual recommends using centers as shown in Fig. 286. It states that this method can be used to determine whether the camshaft is properly aligned relative to the crankshaft. This process involves ensuring that the camshaft rotates smoothly without any misalignment when placed on the centers. If there's an issue with the alignment, further steps such as checking for cracks, wear, or damage to the gears would need to be taken before proceeding w...\n",
      "Time: 11.326684951782227\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 10\n",
      "============================================================\n",
      "According to the instructions in the context, the service manual recommends checking the alignment of the camshaft timing gear with the crankshaft gear before reinstalling it. Specifically, it states:\n",
      "\n",
      "\"The alignment of the camshaft is checked by placing it on centers as shown in Fig. 286.\" \n",
      "\n",
      "This indicates that proper alignment must be ensured before reassembling the components. To confirm the exact method used for alignment, additional details would be needed from the specific section discussi...\n",
      "Time: 7.640256881713867\n",
      "\n",
      "\n",
      "============================================================\n",
      "TOP_K = 20\n",
      "============================================================\n",
      "According to the instructions in the context, the service manual recommends aligning the camshaft timing gear with the crankshaft gear before reassembling the components. Specifically, it states:\n",
      "\n",
      "\"The alignment of the camshaft is checked by placing it on centers as shown in Fig. 286.\"\n",
      "\n",
      "This indicates that proper alignment must be ensured prior to reinstallation to ensure accurate timing and operation of the engine components.\n",
      "Time: 8.891639947891235\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "questions = [\"What is the correct piston ring gaps for a Model T Ford?\",\n",
    "             \"What type of oil does the Ford manual specify for summer versus winter operation?\",\n",
    "             \"How does the service manual recommend aligning the camshaft timing gear with the crankshaft gear during reassembly?\",\n",
    "             ]  # ‚Üê Use a corpus-specific question!\\\n",
    "\n",
    "i = 0\n",
    "if index.ntotal > 0:\n",
    "    for question in questions:\n",
    "        print(f\"\\n\\n{'-'* 70}\\nQuestion {i + 1}: {question}\\n\")\n",
    "        for k in [1, 3, 5, 10, 20]:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"TOP_K = {k}\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "            start = time.time()\n",
    "            answer = rag_query(question, top_k=k)\n",
    "            end = time.time()\n",
    "            \n",
    "            print(answer[:500] + \"...\" if len(answer) > 500 else answer)\n",
    "            print(f\"Time: {end - start}\\n\")\n",
    "        print(\"\\n\\n\\n\")\n",
    "else:\n",
    "    print(\"Please complete the pipeline setup first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 3: Question the corpus CAN'T answer\n",
    "# ==========================================\n",
    "# Does the model admit it doesn't know, or hallucinate?\n",
    "\n",
    "unanswerable_question = \"What is the CEO's favorite color?\"\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    print(f\"Question: {unanswerable_question}\\n\")\n",
    "    answer = rag_query(unanswerable_question, top_k=5, show_context=True)\n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "else:\n",
    "    print(\"Please complete the pipeline setup first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save/Load Your Index\n",
    "\n",
    "For large corpora, you don't want to re-embed every time. Here's how to persist the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_index(filepath: str):\n",
    "    \"\"\"Save FAISS index and chunks to disk.\"\"\"\n",
    "    faiss.write_index(index, f\"{filepath}.faiss\")\n",
    "    with open(f\"{filepath}.chunks\", 'wb') as f:\n",
    "        pickle.dump(all_chunks, f)\n",
    "    print(f\"‚úì Saved index to {filepath}.faiss\")\n",
    "    print(f\"‚úì Saved chunks to {filepath}.chunks\")\n",
    "\n",
    "def load_saved_index(filepath: str):\n",
    "    \"\"\"Load FAISS index and chunks from disk.\"\"\"\n",
    "    global index, all_chunks\n",
    "    index = faiss.read_index(f\"{filepath}.faiss\")\n",
    "    with open(f\"{filepath}.chunks\", 'rb') as f:\n",
    "        all_chunks = pickle.load(f)\n",
    "    print(f\"‚úì Loaded index with {index.ntotal} vectors\")\n",
    "\n",
    "# Save your index\n",
    "if index.ntotal > 0:\n",
    "    save_index(\"my_rag_index\")\n",
    "else:\n",
    "    print(\"No index to save.\")\n",
    "\n",
    "# Later, to load:\n",
    "# load_saved_index(\"my_rag_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "You've built a complete RAG pipeline from scratch! In the next class, we'll:\n",
    "\n",
    "1. **Improve retrieval** with query rewriting and hybrid search\n",
    "2. **Rebuild with LangChain** to see how frameworks abstract these steps\n",
    "3. **Evaluate systematically** with test questions and metrics\n",
    "\n",
    "### Exercises to try:\n",
    "- Vary chunk size (256, 512, 1024) and measure retrieval quality\n",
    "- Try a different embedding model (`BAAI/bge-small-en-v1.5`)\n",
    "- Try a larger LLM (`Qwen/Qwen2.5-3B-Instruct`) and compare answer quality\n",
    "- Ask questions that require combining information from multiple chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: Device Information\n",
    "\n",
    "Run this cell to see detailed information about your compute environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_device_info():\n",
    "    \"\"\"Print detailed information about available compute devices.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DEVICE INFORMATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nEnvironment: {ENVIRONMENT}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # CUDA\n",
    "    print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # MPS\n",
    "    print(f\"\\nMPS available: {torch.backends.mps.is_available()}\")\n",
    "    print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "    \n",
    "    # Current selection\n",
    "    print(f\"\\n‚Üí Selected device: {DEVICE}\")\n",
    "    print(f\"‚Üí Selected dtype: {DTYPE}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print_device_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csAAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
