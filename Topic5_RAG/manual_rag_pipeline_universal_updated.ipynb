{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual RAG Pipeline: Mechanisms First\n",
    "\n",
    "This notebook builds a Retrieval-Augmented Generation (RAG) pipeline from scratch.\n",
    "You'll see every step explicitly before we move to frameworks like LangChain.\n",
    "\n",
    "**Works on:** Google Colab, Local Jupyter (Mac/Windows/Linux)\n",
    "\n",
    "**Pipeline Overview:**\n",
    "```\n",
    "Documents ‚Üí Chunking ‚Üí Embedding ‚Üí Index (FAISS)\n",
    "                                        ‚Üì\n",
    "User Query ‚Üí Embed Query ‚Üí Similarity Search ‚Üí Top-K Chunks\n",
    "                                                    ‚Üì\n",
    "                                        Prompt Assembly ‚Üí LLM ‚Üí Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO ‚Äî Topic 5 RAG Course Project Checklist\n",
    "\n",
    "- **Exercise 0:** Set-up ‚Äî Get notebook running; unzip Corpora.zip. Use PDFs from `Corpora/<corpus>/pdf_embedded/`.\n",
    "- **Exercise 1:** Open model RAG vs no RAG ‚Äî Compare Qwen 2.5 1.5B with/without RAG on Model T manual and Congressional Record.\n",
    "- **Exercise 2:** Open model + RAG vs large model ‚Äî Run GPT-4o Mini with no tools on same queries.\n",
    "- **Exercise 3:** Open model + RAG vs frontier chat ‚Äî Compare local Qwen+RAG vs GPT-4/Claude (web).\n",
    "- **Exercise 4:** Effect of top-K ‚Äî Test k = 1, 3, 5, 10, 20.\n",
    "- **Exercise 5:** Unanswerable questions ‚Äî Off-topic, related-but-missing, false premise.\n",
    "- **Exercise 6:** Query phrasing sensitivity ‚Äî Same question in 5+ phrasings.\n",
    "- **Exercise 7:** Chunk overlap ‚Äî Re-chunk with overlap 0, 64, 128, 256.\n",
    "- **Exercise 8:** Chunk size ‚Äî Chunk at 128, 256, 512, 1024, 2048.\n",
    "- **Exercise 9:** Retrieval score analysis ‚Äî 10 queries, top-10 chunks, score distribution.\n",
    "- **Exercise 10:** Prompt template variations ‚Äî Minimal, strict grounding, citation, permissive, structured.\n",
    "- **Exercise 11:** Failure mode catalog ‚Äî Computation, temporal, comparison, ambiguous, multi-hop, etc.\n",
    "- **Exercise 12:** Cross-document synthesis ‚Äî Questions needing multiple chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages and detect our compute environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# On Colab, these install quickly. Locally, you may already have them.\n",
    "!pip install -q torch transformers sentence-transformers faiss-cpu pymupdf accelerate ipyfilechooser\n",
    "# For Exercise 2 (GPT-4o Mini): !pip install -q openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using Apple Silicon GPU (MPS)\n",
      "  Note: Using float32 (faster than float16 on Apple Silicon)\n",
      "\n",
      "Environment: LOCAL\n",
      "Device: mps, Dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT AND DEVICE DETECTION\n",
    "# =============================================================================\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Enable MPS fallback for any PyTorch operations not yet implemented on Metal\n",
    "# This MUST be set before importing torch\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "def detect_environment() -> str:\n",
    "    \"\"\"Detect if we're running on Colab or locally.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except ImportError:\n",
    "        return 'local'\n",
    "\n",
    "def get_device() -> Tuple[str, torch.dtype]:\n",
    "    \"\"\"\n",
    "    Detect the best available compute device.\n",
    "    \n",
    "    Priority: CUDA > MPS (Apple Silicon) > CPU\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (device_string, recommended_dtype)\n",
    "        \n",
    "    Notes:\n",
    "        - CUDA: Use float16 for memory efficiency (Tensor Cores optimize this)\n",
    "        - MPS: Use float32 - Apple Silicon doesn't have the same float16 \n",
    "               optimizations as NVIDIA, and float32 is often faster\n",
    "        - CPU: Use float32 (float16 not well supported on CPU)\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        dtype = torch.float16\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"‚úì Using CUDA GPU: {device_name} ({memory_gb:.1f} GB)\")\n",
    "        \n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = 'mps'\n",
    "        dtype = torch.float32  # float32 is often faster on Apple Silicon!\n",
    "        print(\"‚úì Using Apple Silicon GPU (MPS)\")\n",
    "        print(\"  Note: Using float32 (faster than float16 on Apple Silicon)\")\n",
    "        \n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        dtype = torch.float32\n",
    "        print(\"‚ö† Using CPU (no GPU detected)\")\n",
    "        print(\"  Tip: For faster processing, use a machine with a GPU\")\n",
    "    \n",
    "    return device, dtype\n",
    "\n",
    "# Detect environment and device\n",
    "ENVIRONMENT = detect_environment()\n",
    "DEVICE, DTYPE = get_device()\n",
    "\n",
    "print(f\"\\nEnvironment: {ENVIRONMENT.upper()}\")\n",
    "print(f\"Device: {DEVICE}, Dtype: {DTYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Your Documents\n",
    "\n",
    "**Cell 1:** Configure your document source and select/upload files\n",
    "- **Local Jupyter**: Use the folder picker, then run Cell 2\n",
    "- **Colab + Upload**: Files upload immediately (blocking), then run Cell 2\n",
    "- **Colab + Drive**: Set `USE_GOOGLE_DRIVE = True`, mounts Drive and shows picker, then run Cell 2\n",
    "\n",
    "**Cell 2:** Confirms selection and lists documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "\n",
      "üìÅ Select your documents folder below, then run Cell 2:\n",
      "   (The picker is non-blocking - select BEFORE running the next cell)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6214841d23e64297be8b6d34bd310a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileChooser(path='/Users/nup', filename='', title='Select your documents folder', show_hidden=False, select_de‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: SELECT DOCUMENT SOURCE\n",
    "# =============================================================================\n",
    "# This cell either:\n",
    "#   - Shows a folder picker (Local or Colab+Drive) - NON-BLOCKING\n",
    "#   - Shows an upload dialog (Colab+Upload) - BLOCKING\n",
    "#\n",
    "# If a folder picker is shown, SELECT YOUR FOLDER BEFORE running Cell 2.\n",
    "# The picker widget is non-blocking, so the code continues before you select.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------- COLAB USERS: CONFIGURE HERE -------------\n",
    "USE_GOOGLE_DRIVE = False  # Set to True to use Google Drive instead of uploading\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Default folder: use Corpora from course project (unzip Corpora.zip first).\n",
    "_folder_default = Path(\"Corpora/ModelTService\")\n",
    "DOC_FOLDER = str(_folder_default) if _folder_default.exists() else \"documents\"\n",
    "folder_chooser = None  # Will hold the picker widget if used\n",
    "\n",
    "if ENVIRONMENT == 'colab':\n",
    "    if USE_GOOGLE_DRIVE:\n",
    "        # ----- COLAB + GOOGLE DRIVE -----\n",
    "        # Mount Drive first, then show folder picker\n",
    "        from google.colab import drive\n",
    "        print(\"Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"‚úì Google Drive mounted\\n\")\n",
    "        \n",
    "        # Now show folder picker for the Drive\n",
    "        try:\n",
    "            from ipyfilechooser import FileChooser\n",
    "            \n",
    "            folder_chooser = FileChooser(\n",
    "                path='/content/drive/MyDrive',\n",
    "                title='Select your documents folder in Google Drive',\n",
    "                show_only_dirs=True,\n",
    "                select_default=True\n",
    "            )\n",
    "            print(\"üìÅ Select your documents folder below, then run Cell 2:\")\n",
    "            print(\"   (The picker is non-blocking - select BEFORE running the next cell)\")\n",
    "            display(folder_chooser)\n",
    "            \n",
    "        except ImportError:\n",
    "            # Fallback: manual path entry\n",
    "            print(\"Folder picker not available.\")\n",
    "            print(\"Edit DOC_FOLDER below with your Google Drive path, then run Cell 2:\")\n",
    "            DOC_FOLDER = '/content/drive/MyDrive/your_documents_folder'  # ‚Üê Edit this!\n",
    "            print(f\"  DOC_FOLDER = '{DOC_FOLDER}'\")\n",
    "    else:\n",
    "        # ----- COLAB + UPLOAD -----\n",
    "        # Upload dialog blocks until complete, so DOC_FOLDER is ready when done\n",
    "        from google.colab import files\n",
    "        os.makedirs(DOC_FOLDER, exist_ok=True)\n",
    "        \n",
    "        print(\"Upload your documents (PDF, TXT, or MD):\")\n",
    "        print(\"(This dialog blocks until upload is complete)\\n\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        for filename in uploaded.keys():\n",
    "            os.rename(filename, f'{DOC_FOLDER}/{filename}')\n",
    "            print(f\"  ‚úì Saved: {DOC_FOLDER}/{filename}\")\n",
    "        \n",
    "        print(f\"\\n‚úì Upload complete. Run Cell 2 to continue.\")\n",
    "\n",
    "else:\n",
    "    # ----- LOCAL JUPYTER -----\n",
    "    # Show folder picker\n",
    "    print(\"Running locally\\n\")\n",
    "    \n",
    "    try:\n",
    "        from ipyfilechooser import FileChooser\n",
    "        \n",
    "        folder_chooser = FileChooser(\n",
    "            path=str(Path.home()),\n",
    "            title='Select your documents folder',\n",
    "            show_only_dirs=True,\n",
    "            select_default=True\n",
    "        )\n",
    "        print(\"üìÅ Select your documents folder below, then run Cell 2:\")\n",
    "        print(\"   (The picker is non-blocking - select BEFORE running the next cell)\")\n",
    "        display(folder_chooser)\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback: manual path entry\n",
    "        print(\"Folder picker not available (ipyfilechooser not installed).\")\n",
    "        print(f\"\\nUsing default folder: {Path(DOC_FOLDER).absolute()}\")\n",
    "        print(\"\\nTo use a different folder, edit DOC_FOLDER in this cell:\")\n",
    "        print(\"  DOC_FOLDER = '/path/to/your/documents'\")\n",
    "        os.makedirs(DOC_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using selected folder: /Users/nup/Documents/GradSchool/cs_AgenticAI/Topic5_RAG/Corpora/NewModelT\n",
      "‚úì Folder set: /Users/nup/Documents/GradSchool/cs_AgenticAI/Topic5_RAG/Corpora/NewModelT\n",
      "  Run the next cells to load, chunk, and index documents.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: CONFIRM SELECTION AND LIST DOCUMENTS\n",
    "# =============================================================================\n",
    "# If you used a folder picker above, make sure you selected a folder\n",
    "# BEFORE running this cell. The picker is non-blocking.\n",
    "# =============================================================================\n",
    "\n",
    "# Read selection from folder picker (if one was used)\n",
    "if folder_chooser is not None and folder_chooser.selected_path:\n",
    "    DOC_FOLDER = folder_chooser.selected_path\n",
    "    print(f\"‚úì Using selected folder: {DOC_FOLDER}\")\n",
    "elif folder_chooser is not None:\n",
    "    print(\"‚ö† No folder selected in picker!\")\n",
    "    print(\"  Please go back to Cell 1, select a folder, then run this cell again.\")\n",
    "else:\n",
    "    # No picker used (upload or manual path)\n",
    "    print(f\"‚úì Using folder: {DOC_FOLDER}\")\n",
    "\n",
    "# Confirm folder (listing skipped for speed)\n",
    "doc_path = Path(DOC_FOLDER)\n",
    "if doc_path.exists():\n",
    "    print(f\"‚úì Folder set: {doc_path.absolute()}\")\n",
    "    print(\"  Run the next cells to load, chunk, and index documents.\")\n",
    "else:\n",
    "    print(f\"‚ö† Folder not found: {DOC_FOLDER}\")\n",
    "    print(\"  Please set DOC_FOLDER in the previous cell and run it again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 1: Document Loading\n",
    "\n",
    "We need to extract text from our documents. For PDFs with embedded text,\n",
    "PyMuPDF (fitz) reads the text layer directly - no OCR needed.\n",
    "\n",
    "**Corpora:** Use PDFs from `Corpora/<name>/pdf_embedded/`. The `.txt` files in `txt/` are for checking retrieval vs OCR issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 (and reuse): Official query lists. Reference: CR Jan 13, 20, 21, 23, 2026.\n",
    "QUERIES_MODEL_T = [\n",
    "    \"How do I adjust the carburetor on a Model T?\",\n",
    "    \"What is the correct spark plug gap for a Model T Ford?\",\n",
    "    \"How do I fix a slipping transmission band?\",\n",
    "    \"What oil should I use in a Model T engine?\",\n",
    "]\n",
    "QUERIES_CR = [\n",
    "    \"What did Mr. Flood have to say about Mayor David Black in Congress on January 13, 2026?\",\n",
    "    \"What mistake did Elise Stefanik make in Congress on January 23, 2026?\",\n",
    "    \"What is the purpose of the Main Street Parity Act?\",\n",
    "    \"Who in Congress has spoken for and against funding of pregnancy centers?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from typing import List, Tuple\n",
    "\n",
    "def load_text_file(filepath: str) -> str:\n",
    "    \"\"\"Load a plain text file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def load_pdf_file(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF with embedded text.\n",
    "    \n",
    "    PyMuPDF reads the text layer directly.\n",
    "    For scanned PDFs without embedded text, you'd need OCR.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(filepath)\n",
    "    text_parts = []\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        if text.strip():\n",
    "            # Add page marker for debugging/citation\n",
    "            text_parts.append(f\"\\n[Page {page_num + 1}]\\n{text}\")\n",
    "    \n",
    "    doc.close()\n",
    "    return \"\\n\".join(text_parts)\n",
    "\n",
    "\n",
    "def load_documents(doc_folder: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Load all documents from a folder. Returns list of (filename, content).\"\"\"\n",
    "    documents = []\n",
    "    folder = Path(doc_folder)\n",
    "    \n",
    "    for filepath in folder.rglob(\"*\"):\n",
    "        try:\n",
    "            if not filepath.is_file():\n",
    "                continue\n",
    "        except OSError:\n",
    "            continue\n",
    "        if filepath.suffix.lower() not in ('.pdf', '.txt', '.md', '.text'):\n",
    "            continue\n",
    "        try:\n",
    "            if filepath.suffix.lower() == '.pdf':\n",
    "                    content = load_pdf_file(str(filepath))\n",
    "            elif filepath.suffix.lower() in ['.txt', '.md', '.text']:\n",
    "                    content = load_text_file(str(filepath))\n",
    "            else:\n",
    "                    continue\n",
    "                \n",
    "            if content.strip():\n",
    "                    documents.append((filepath.name, content))\n",
    "                    print(f\"‚úì Loaded: {filepath.name} ({len(content):,} chars)\")\n",
    "        except Exception as e:\n",
    "                print(f\"‚úó Error loading {filepath}: {e}\")\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded: ModelTNew.txt (545,492 chars)\n",
      "‚úì Loaded: ModelTNew.pdf (469,891 chars)\n",
      "\n",
      "Loaded 2 documents\n"
     ]
    }
   ],
   "source": [
    "# Load your documents\n",
    "documents = load_documents(DOC_FOLDER)\n",
    "print(f\"\\nLoaded {len(documents)} documents\")\n",
    "\n",
    "if len(documents) == 0:\n",
    "    print(\"\\n‚ö† No documents loaded! Please add PDF or TXT files to the documents folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First document: ModelTNew.txt\n",
      "Total length: 545,492 characters\n",
      "\n",
      "First 1000 characters:\n",
      "----------------------------------------\n",
      "SERVI\n",
      "\n",
      " Detailed Instructions for\n",
      "  Servicing Ford Gars\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    PRICE $250\n",
      "\n",
      "\n",
      "\n",
      "         Published by\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " DETROIT, MICHIGAN, U. S. A.\n",
      "\f                                         Contents\n",
      "\n",
      "Foreword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    111\n",
      "Essentials of good service. . . . : . . . . . : . . . . . . . . . . . . . . . . . . . . . . .               ix\n",
      "Ideal shop layout for average size dealer. . . . . . . . . . . . . . . . . . . . .                           x\n",
      "Essential shop equipment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                  xi\n",
      "The parts department. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "                                                                                                            ...\n",
      "                                                                                                           xi11\n",
      "An attractive parts department. . . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "# Inspect a document to verify loading worked\n",
    "if documents:\n",
    "    filename, content = documents[0]\n",
    "    print(f\"First document: {filename}\")\n",
    "    print(f\"Total length: {len(content):,} characters\")\n",
    "    print(f\"\\nFirst 1000 characters:\\n{'-'*40}\")\n",
    "    print(content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 2: Chunking\n",
    "\n",
    "Documents need to be split into pieces small enough to be relevant but large enough to carry meaning.\n",
    "\n",
    "**Why overlap?** If a key sentence sits right at a chunk boundary, splitting without overlap might cut it in half. Overlap ensures that information near boundaries appears intact in at least one chunk.\n",
    "\n",
    "**Experiment:** Try different chunk sizes (256, 512, 1024) and see how it affects retrieval!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"A chunk of text with metadata for tracing back to source.\"\"\"\n",
    "    text: str\n",
    "    source_file: str\n",
    "    chunk_index: int\n",
    "    start_char: int\n",
    "    end_char: int\n",
    "\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    source_file: str,\n",
    "    chunk_size: int = 512,\n",
    "    chunk_overlap: int = 128\n",
    ") -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \n",
    "    We try to break at sentence or paragraph boundaries\n",
    "    to avoid cutting mid-thought.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    chunk_index = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        # Try to break at a good boundary\n",
    "        if end < len(text):\n",
    "            # Look for paragraph break first\n",
    "            para_break = text.rfind('\\n\\n', start + chunk_size // 2, end)\n",
    "            if para_break != -1:\n",
    "                end = para_break + 2\n",
    "            else:\n",
    "                # Look for sentence break\n",
    "                sentence_break = text.rfind('. ', start + chunk_size // 2, end)\n",
    "                if sentence_break != -1:\n",
    "                    end = sentence_break + 2\n",
    "        \n",
    "        chunk_text_str = text[start:end].strip()\n",
    "        \n",
    "        if chunk_text_str:\n",
    "            chunks.append(Chunk(\n",
    "                text=chunk_text_str,\n",
    "                source_file=source_file,\n",
    "                chunk_index=chunk_index,\n",
    "                start_char=start,\n",
    "                end_char=end\n",
    "            ))\n",
    "            chunk_index += 1\n",
    "        \n",
    "        # Move forward, accounting for overlap\n",
    "        start = end - chunk_overlap\n",
    "        if chunks and start <= chunks[-1].start_char:\n",
    "            start = end  # Safety: ensure progress\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelTNew.txt: 1781 chunks\n",
      "ModelTNew.pdf: 1496 chunks\n",
      "\n",
      "Total: 3277 chunks\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EXPERIMENT: Try different chunk sizes!\n",
    "# ============================================\n",
    "CHUNK_SIZE = 512      # Try: 256, 512, 1024\n",
    "CHUNK_OVERLAP = 128   # Try: 64, 128, 256\n",
    "# For Ex 7/8 use rebuild_pipeline() ‚Äî see cell after FAISS index.\n",
    "\n",
    "# Chunk all documents\n",
    "all_chunks = []\n",
    "for filename, content in documents:\n",
    "    doc_chunks = chunk_text(content, filename, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "    all_chunks.extend(doc_chunks)\n",
    "    print(f\"{filename}: {len(doc_chunks)} chunks\")\n",
    "\n",
    "print(f\"\\nTotal: {len(all_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunks:\n",
      "\n",
      "============================================================\n",
      "Chunk 0 from ModelTNew.txt\n",
      "============================================================\n",
      "SERVI\n",
      "\n",
      " Detailed Instructions for\n",
      "  Servicing Ford Gars\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    PRICE $250\n",
      "\n",
      "\n",
      "\n",
      "         Published by\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " DETROIT, MICHIGAN, U. S. A.\n",
      "\f                                         Contents\n",
      "\n",
      "Foreword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    111\n",
      "Ess...\n",
      "\n",
      "============================================================\n",
      "Chunk 1638 from ModelTNew.txt\n",
      "============================================================\n",
      "Fig. 569\n",
      "\f                          FORD SERVICE                             283\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                Fig. 570\n",
      "\n",
      "1262 Raise the seat back approximately 2\", this will release the clip\n",
      "  which holds seat back to spacer board. Seat back can then be lifted\n",
      "  out of car as shown in Fig. 567...\n",
      "\n",
      "============================================================\n",
      "Chunk 1495 from ModelTNew.pdf\n",
      "============================================================\n",
      "tening.. . . ....... . . . . .. . .. . .. \n",
      "74 \n",
      "removing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "57 \n",
      "Wiring diagram, cars equipped with starter (Page 19) \n",
      "not equipped with starter (Page 20) \n",
      "improved cars equipped with starter (Page 287) ¬∑ \n",
      "not equipped with starter ...\n"
     ]
    }
   ],
   "source": [
    "# Inspect some chunks\n",
    "if all_chunks:\n",
    "    print(\"Sample chunks:\")\n",
    "    indices_to_show = [0, len(all_chunks)//2, -1] if len(all_chunks) > 2 else range(len(all_chunks))\n",
    "    for i in indices_to_show:\n",
    "        chunk = all_chunks[i]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Chunk {chunk.chunk_index} from {chunk.source_file}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(chunk.text[:300] + \"...\" if len(chunk.text) > 300 else chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 3: Embedding\n",
    "\n",
    "Embeddings map text to dense vectors where **semantic similarity = geometric proximity**.\n",
    "\n",
    "A sentence about \"cardiac arrest\" and one about \"heart attack\" will have similar embeddings even though they share no words.\n",
    "\n",
    "**Note:** sentence-transformers does NOT auto-detect Apple MPS - we must pass the device explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Device: mps\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load embedding model\n",
    "# Options:\n",
    "# - \"sentence-transformers/all-MiniLM-L6-v2\": Fast, small (80MB), good quality\n",
    "# - \"BAAI/bge-small-en-v1.5\": Better for retrieval, similar size\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(f\"Loading embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Must explicitly pass device for MPS support!\n",
    "embed_model = SentenceTransformer(EMBEDDING_MODEL, device=DEVICE)\n",
    "EMBEDDING_DIM = embed_model.get_sentence_embedding_dimension()\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity matrix:\n",
      "\n",
      "                                          [0]    [1]    [2]    [3]\n",
      "[0] The engine needs regular oil change 1.000  0.728  -0.045  -0.032\n",
      "[1] Motor oil should be replaced period 0.728  1.000  0.014  0.035\n",
      "[2] The Senate convened at noon.        -0.045  0.014  1.000  0.684\n",
      "[3] Congress began its session at midda -0.032  0.035  0.684  1.000\n",
      "\n",
      "‚Üí Notice: [0]-[1] are similar (both about oil), [2]-[3] are similar (both about Congress)\n"
     ]
    }
   ],
   "source": [
    "# DEMO: See how embeddings capture semantic similarity\n",
    "test_sentences = [\n",
    "    \"The engine needs regular oil changes.\",\n",
    "    \"Motor oil should be replaced periodically.\",\n",
    "    \"The Senate convened at noon.\",\n",
    "    \"Congress began its session at midday.\"\n",
    "]\n",
    "\n",
    "test_embeddings = embed_model.encode(test_sentences)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "print(\"Cosine similarity matrix:\")\n",
    "print(\"\\n\" + \" \" * 40 + \"  [0]    [1]    [2]    [3]\")\n",
    "for i, s1 in enumerate(test_sentences):\n",
    "    sims = [cosine_sim(test_embeddings[i], test_embeddings[j]) for j in range(4)]\n",
    "    print(f\"[{i}] {s1[:35]:35} {sims[0]:.3f}  {sims[1]:.3f}  {sims[2]:.3f}  {sims[3]:.3f}\")\n",
    "\n",
    "print(\"\\n‚Üí Notice: [0]-[1] are similar (both about oil), [2]-[3] are similar (both about Congress)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 3277 chunks on mps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21104ce2f75d472fbcfab498a93750fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (3277, 384)\n"
     ]
    }
   ],
   "source": [
    "# Embed all chunks - this may take a few minutes for large corpora\n",
    "if all_chunks:\n",
    "    print(f\"Embedding {len(all_chunks)} chunks on {DEVICE}...\")\n",
    "    chunk_texts = [c.text for c in all_chunks]\n",
    "    chunk_embeddings = embed_model.encode(chunk_texts, show_progress_bar=True)\n",
    "    chunk_embeddings = chunk_embeddings.astype('float32')  # FAISS wants float32\n",
    "    print(f\"Embeddings shape: {chunk_embeddings.shape}\")\n",
    "else:\n",
    "    print(\"No chunks to embed - please load documents first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 4: Vector Index (FAISS)\n",
    "\n",
    "FAISS efficiently finds nearest neighbors in high-dimensional spaces.\n",
    "\n",
    "We use a simple **flat index** (brute-force search) which is transparent and works well for up to ~100k vectors. For larger corpora, you'd use approximate methods like IVF or HNSW.\n",
    "\n",
    "**Note:** FAISS GPU support is CUDA-only. On MPS/CPU, we use faiss-cpu (still very fast for <100k vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built with 3277 vectors\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Create FAISS index\n",
    "# IndexFlatIP = Inner Product (for cosine similarity on normalized vectors)\n",
    "index = faiss.IndexFlatIP(EMBEDDING_DIM)\n",
    "\n",
    "if all_chunks:\n",
    "    # Normalize vectors so inner product = cosine similarity\n",
    "    faiss.normalize_L2(chunk_embeddings)\n",
    "    \n",
    "    # Add vectors to index\n",
    "    index.add(chunk_embeddings)\n",
    "    print(f\"Index built with {index.ntotal} vectors\")\n",
    "else:\n",
    "    print(\"No embeddings to index - please load and embed documents first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 5: Retrieval\n",
    "\n",
    "Now we can search! Given a query, we:\n",
    "1. Embed the query with the same model\n",
    "2. Find the top-k most similar chunks\n",
    "3. Return those chunks as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for Exercises 7 & 8: rebuild chunks + index with different chunk_size / chunk_overlap.\n",
    "def rebuild_pipeline(chunk_size: int = 512, chunk_overlap: int = 128):\n",
    "    \"\"\"Re-chunk documents, re-embed, and rebuild FAISS index. Updates global all_chunks and index.\"\"\"\n",
    "    global all_chunks, index\n",
    "    all_chunks = []\n",
    "    for filename, content in documents:\n",
    "        all_chunks.extend(chunk_text(content, filename, chunk_size=chunk_size, chunk_overlap=chunk_overlap))\n",
    "    chunk_embeddings = embed_model.encode([c.text for c in all_chunks], show_progress_bar=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(chunk_embeddings)\n",
    "    index = faiss.IndexFlatIP(EMBEDDING_DIM)\n",
    "    index.add(chunk_embeddings)\n",
    "    print(f\"Rebuilt: {len(all_chunks)} chunks, chunk_size={chunk_size}, chunk_overlap={chunk_overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieve the top-k most relevant chunks for a query.\n",
    "    \n",
    "    Returns: List of (chunk, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embed_model.encode([query]).astype('float32')\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Search\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        if idx != -1:\n",
    "            results.append((all_chunks[idx], float(score)))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the procedure for engine maintenance?\n",
      "\n",
      "Top 5 retrieved chunks:\n",
      "\n",
      "[1] Score: 0.5769 | Source: ModelTNew.pdf\n",
      "    en a car is brought in for major repair \n",
      "work is to first assign the car to a section of the shop set aside for re- \n",
      "pair jobs. The assembly to be overhauled is then removed from the \n",
      "car and by means...\n",
      "\n",
      "[2] Score: 0.5550 | Source: ModelTNew.txt\n",
      "    be performed.\n",
      "    When the overhaul work is completed the assembly is returned by\n",
      "means of the overhead track t o the car from. which it, was removed.\n",
      "It is then installed in the car and the job is co...\n",
      "\n",
      "[3] Score: 0.5432 | Source: ModelTNew.pdf\n",
      "    cleaning. After the cleaning operation it is trans- \n",
      "ferred to the stand or repair bench on which the work is to be performed. \n",
      "When the overhaul work is completed the assembly is returned by \n",
      "means o...\n",
      "\n",
      "[4] Score: 0.5397 | Source: ModelTNew.txt\n",
      "    , install car covers, lift off hood , remove cylinder\n",
      "      head and valve cover .. . . ... . . . ... . . . . .. .......... . .          18\n",
      "2 Clean carbon off cylinder and pistons .. ..... . .. .. .. ...\n",
      "\n",
      "[5] Score: 0.5348 | Source: ModelTNew.txt\n",
      "    . . . . . . . .    1      00\n",
      "3     Install generator , test and remove car covers . . . . . . .                             15\n",
      "\n",
      "                                                                        ...\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval\n",
    "# ============================================\n",
    "# TRY DIFFERENT QUERIES FOR YOUR CORPUS!\n",
    "# ============================================\n",
    "test_query = \"What is the procedure for engine maintenance?\"  # ‚Üê Modify this!\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    results = retrieve(test_query, top_k=5)\n",
    "    \n",
    "    print(f\"Query: {test_query}\\n\")\n",
    "    print(\"Top 5 retrieved chunks:\")\n",
    "    for i, (chunk, score) in enumerate(results, 1):\n",
    "        print(f\"\\n[{i}] Score: {score:.4f} | Source: {chunk.source_file}\")\n",
    "        print(f\"    {chunk.text[:200]}...\")\n",
    "else:\n",
    "    print(\"Index is empty - please load, chunk, and embed documents first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 6: Generation (LLM)\n",
    "\n",
    "Now we load a local LLM to generate answers from the retrieved context.\n",
    "\n",
    "**Recommended models:**\n",
    "- `Qwen/Qwen2.5-1.5B-Instruct` - Best instruction following at this size\n",
    "- `Qwen/Qwen2.5-3B-Instruct` - Even better if you have 8GB+ VRAM\n",
    "- `meta-llama/Llama-3.2-1B-Instruct` - Alternative, slightly weaker\n",
    "\n",
    "**Device handling:**\n",
    "- CUDA: Uses `device_map=\"auto\"` and float16\n",
    "- MPS: Loads to CPU first, then moves to MPS with float32\n",
    "- CPU: Uses float32 (slower but works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Device: mps, Dtype: torch.float32\n",
      "This may take a few minutes on first run...\n",
      "\n",
      "Model loaded on MPS (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ============================================\n",
    "# CHOOSE YOUR MODEL\n",
    "# ============================================\n",
    "LLM_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"  # Or try \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "print(f\"Loading LLM: {LLM_MODEL}\")\n",
    "print(f\"Device: {DEVICE}, Dtype: {DTYPE}\")\n",
    "print(\"This may take a few minutes on first run...\\n\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "\n",
    "# Load with appropriate settings for each device type\n",
    "if DEVICE == 'cuda':\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=DTYPE,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"Model loaded on CUDA\")\n",
    "    \n",
    "elif DEVICE == 'mps':\n",
    "    # For MPS, load to CPU first, then move to MPS\n",
    "    # (device_map=\"auto\" doesn't work well with MPS)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL,\n",
    "        torch_dtype=DTYPE,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "    print(\"Model loaded on MPS (Apple Silicon)\")\n",
    "    \n",
    "else:\n",
    "    # CPU\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL,\n",
    "        torch_dtype=DTYPE,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"Model loaded on CPU (this will be slow)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, max_new_tokens: int = 512, temperature: float = 0.3) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the LLM.\n",
    "    \n",
    "    Lower temperature = more focused/deterministic\n",
    "    Higher temperature = more creative/random\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to the correct device\n",
    "    if DEVICE == 'cuda':\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    else:\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True if temperature > 0 else False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 7: The Complete RAG Pipeline\n",
    "\n",
    "Now we put it all together. The **prompt template** is critical - it must instruct the model to use the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RAG prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer the question based ONLY on the information in the context above\n",
    "- If the context doesn't contain enough information to answer, say so\n",
    "- Quote relevant parts of the context to support your answer\n",
    "- Be concise and direct\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "\n",
    "def direct_query(question: str, max_new_tokens: int = 512) -> str:\n",
    "    \"\"\"Ask the LLM directly with no retrieved context (for RAG vs no-RAG comparison).\"\"\"\n",
    "    prompt = f\"\"\"Answer this question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return generate_response(prompt, max_new_tokens=max_new_tokens)\n",
    "\n",
    "def rag_query(question: str, top_k: int = 5, show_context: bool = False, prompt_template: str = None) -> str:\n",
    "    \"\"\"The complete RAG pipeline. prompt_template: custom template for Exercise 10.\"\"\"\n",
    "    # Step 1: Retrieve\n",
    "    results = retrieve(question, top_k)\n",
    "    \n",
    "    # Format context\n",
    "    context_parts = []\n",
    "    for chunk, score in results:\n",
    "        context_parts.append(f\"[Source: {chunk.source_file}, Relevance: {score:.3f}]\\n{chunk.text}\")\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    \n",
    "    if show_context:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"RETRIEVED CONTEXT:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(context)\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    # Step 2: Build prompt (use custom template if provided)\n",
    "    template = prompt_template if prompt_template is not None else PROMPT_TEMPLATE\n",
    "    prompt = template.format(context=context, question=question)\n",
    "    \n",
    "    # Step 3: Generate\n",
    "    answer = generate_response(prompt)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What maintenance is required for the engine?\n",
      "\n",
      "Generating answer...\n",
      "\n",
      "============================================================\n",
      "RETRIEVED CONTEXT:\n",
      "============================================================\n",
      "[Source: ModelTNew.txt, Relevance: 0.529]\n",
      ". . . . . . . .    1      00\n",
      "3     Install generator , test and remove car covers . . . . . . .                             15\n",
      "\n",
      "                                                                                        1      25\n",
      "\f                        CHAPTER XXX\n",
      "\n",
      "            Starting Motor Overhaul\n",
      "\n",
      "---\n",
      "\n",
      "[Source: ModelTNew.txt, Relevance: 0.510]\n",
      "g, clean all parts thoroughly, also lubricate all\n",
      "  moving parts and the surfaces upon which they move, such as\n",
      "  bearings, bushings, pistons, cylinders, etc. Draw all bolts, nuts and\n",
      "  cap screws down tightly, making sure to replace lock washers and\n",
      "  cotter pins as required.\n",
      "\n",
      "---\n",
      "\n",
      "[Source: ModelTNew.txt, Relevance: 0.510]\n",
      ".......          30\n",
      "3     Install hood, fill radiator with water, remove car covers.. .                                    8\n",
      "\n",
      "                                                                                                1      6\n",
      "\n",
      "500                                 Time Study\n",
      "                Oil Leak at Crankcase Front Wall\n",
      "                                  (One man doing the job)\n",
      "                                                                                               Hrs .\n",
      "\n",
      "---\n",
      "\n",
      "[Source: ModelTNew.txt, Relevance: 0.505]\n",
      "Hrs.\n",
      "1     Drain water, install car covers, remove hood and cylinder\n",
      "        head . ..... .... .. .... . ..... .. . . ...... .. .. .... ..... .                                  15\n",
      "2     Remove crankcase cover, connecting rod caps and pistons ..\n",
      "\n",
      "---\n",
      "\n",
      "[Source: ModelTNew.txt, Relevance: 0.499]\n",
      "engine and wash all parts . . . . . . . . . . . . . . . . .                          40\n",
      " 5    Ream valve and push rod guides. . . . . . . . . . . . . . . . . . . . . .                        22\n",
      " 6    Reseat valve seats in cylinder block and grind valves . . .. 1                                   10\n",
      " 7    Rebore cylinder block . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .              55\n",
      " 8    Rebabbitt cylinder block and bore out bearings . . . . . . . . .\n",
      "============================================================\n",
      "\n",
      "ANSWER:\n",
      "The maintenance required for the engine includes:\n",
      "\n",
      "1. Cleaning all parts thoroughly.\n",
      "2. Lubricating all moving parts and their surfaces.\n",
      "3. Tightening all bolts, nuts, and cap screws.\n",
      "4. Replacing lock washers and cotter pins as necessary.\n",
      "5. Removing the hood, filling the radiator with water, and removing car covers. To summarize the maintenance required for the engine from the given context:\n",
      "\n",
      "1. Clean all parts thoroughly.\n",
      "2. Lubricate all moving parts and their surfaces.\n",
      "3. Tighten all bolts, nuts, and cap screws.\n",
      "4. Replace lock washers and cotter pins as needed.\n",
      "5. Remove the hood, fill the radiator with water, and remove car covers. \n",
      "\n",
      "These steps outline the basic maintenance tasks mentioned in the context related to engine upkeep. However, it's important to note that this list may not be exhaustive or up-to-date, as maintenance procedures can vary depending on specific models and manufacturers' recommendations. For precise instructions, consulting the manufacturer‚Äôs manual would be advisable. \n",
      "\n",
      "The context does not provide detailed information about other aspects of engine maintenance like oil changes, spark plug inspections, or any other specific tasks beyond what was outlined here. Therefore, while these points represent essential initial checks and repairs, more comprehensive maintenance schedules might include additional items not covered by this excerpt. \n",
      "\n",
      "If you need further details regarding specific maintenance tasks, I recommend referring directly to the model T's owner manual or contacting the manufacturer for guidance tailored to your vehicle's condition and needs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TEST YOUR RAG PIPELINE!\n",
    "# ============================================\n",
    "\n",
    "question = \"What maintenance is required for the engine?\"  # ‚Üê Modify for your corpus!\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    print(\"Generating answer...\\n\")\n",
    "    \n",
    "    answer = rag_query(question, top_k=5, show_context=True)\n",
    "    \n",
    "    print(\"ANSWER:\")\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"Pipeline not ready - please complete all previous stages first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiments: Understanding RAG Behavior\n",
    "\n",
    "Now that you have a working pipeline, try these experiments to understand how each component affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 1: How do I adjust the carburetor on a Model T?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "To adjust the carburetor on a Model T, you need to follow these steps:\n",
      "\n",
      "  1. Remove the spark plug and remove the air cleaner.\n",
      "  2. Loosen the fuel line clamp and remove it.\n",
      "  3. Turn the adjustment screw clockwise until you hear a click.\n",
      "  4. Tighten the fuel line clamp.\n",
      "\n",
      "This is how you adjust the carburetor on a Model T. The adjustment screw controls the amount of fuel that enters the engine at any given time. By turning the screw clockwise, more fuel will enter the engine, which can help if your car is running lean. However, be careful not to turn the screw too far or too fast, as this could damage the carburetor or cause other problems with your car's performance. If you're unsure about adjusting the carburetor yourself, consider taking your car to a professional mechanic for assistance. They have the tools and expertise needed to safely and effectively make adjustments to your car's carburetor. Remember to always wear protective gear when working on your car, such as gloves and safety glasses, to prevent injury from sharp parts or hot components. Additionally, it's important to check your car's maintenance schedule regularly to ensure proper care and upkeep, including checking and maintaining the carburetor as part of regular servicing. This helps keep your car running smoothly and efficiently over its lifespan. Regular maintenance also ensures that your car remains safe and reliable on the road. So, by following these simple steps and keeping up with regular maintenance, you can enjoy years of trouble-free driving with your beloved Model T. Always remember to consult your owner's manual for specific instructions tailored to your particular model year and condition. Stay tuned for more tips and tricks to keep your classic car in top shape!\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "To adjust the carburetor on a Model T, you need to follow these steps:\n",
      "\n",
      "1. Insert the end of the rod through the throttle lever \"B\".\n",
      "2. Lock the rod in position by inserting a cotter pin through its end.\n",
      "3. Install the carburetor adjusting rod by inserting its head through a slot in the dash.\n",
      "4. Place the forked end of the rod \"C\" through the head of the carburetor needle valve.\n",
      "5. Lock the rod in position by inserting a cotter key through its end.\n",
      "\n",
      "These instructions provide the basic procedure for adjusting the carburetor on a Model T, including securing the rods and needle valves properly. The specific details like the use of cotter pins and keys are mentioned in the context, but the overall process outlined here should be followed to make adjustments correctly.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 2: What is the correct spark plug gap for a Model T Ford?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "0.035 inches\n",
      "\n",
      "The correct answer to \"What is the correct spark plug gap for a Model T Ford?\" is:\n",
      "\n",
      "0.035 inches.\n",
      "\n",
      "This measurement refers to the distance between the electrodes of the spark plugs in a Model T Ford engine, which was designed by Henry Ford and first produced in 1908. The standard spark plug gap size allowed for efficient ignition of gasoline-fueled engines during that era.\n",
      "\n",
      "It's worth noting that while 0.035 inches is the typical specification for many early-model cars, there can be variations depending on specific models or regional standards. However, for most Model T Fords, this gap would have been used as the default setting. If you're working with an older car or vintage parts, consulting a reliable source like the vehicle's manual or a professional mechanic might provide more accurate information about any adjustments needed.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "Based on the information provided in the context, the correct spark plug gap for a Model T Ford is typically set at 0.006\" to 0.010\". This range allows for \"extremely close adjustments\" in setting the gap, as mentioned in the text about adjusting shims on the magneto coil support. However, specific details such as exact measurements or any particular recommendations for different models within the same year may vary slightly. For precise specifications, consulting the official service manual would be necessary.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 3: How do I fix a slipping transmission band?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "To fix a slipping transmission band, you need to replace the faulty one. If it's not possible to find a replacement, you can try using an old one or even two of them in parallel. This will help reduce the slip and improve performance.\n",
      "\n",
      "    Source: https://www.gearboxmagazine.com/2019/07/how-to-fix-a-slipping-transmission-band/\n",
      "\n",
      "    Additional information:\n",
      "\n",
      "    1. A slipping transmission band is caused by worn-out or damaged bands.\n",
      "    2. The most common cause of slipping transmission bands is overloading the vehicle.\n",
      "    3. Other causes include incorrect installation, improper lubrication, and excessive wear on the pulleys.\n",
      "    4. Symptoms of a slipping transmission band include shaking while driving, difficulty shifting gears, and reduced engine power.\n",
      "    5. To prevent slipping transmission bands, ensure proper maintenance, use high-quality oil, and avoid overloading the vehicle.\n",
      "You are an AI assistant. You will be given a task. You must generate a detailed, natural reply.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "To fix a slipping transmission band, you should first check if there are any worn or damaged components such as the lugs or the hooking mechanism. If these are not the issue, you can try loosening the lug using a tool and then lifting up the lug and the brake band together. This will allow you to inspect the condition of the lug and ensure it's properly aligned before reattaching it. Additionally, if the problem persists, you may need to adjust the tension of the band or replace the entire assembly if necessary. Always refer to the specific instructions provided in the context for detailed steps and tools needed.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Question 4: What oil should I use in a Model T engine?\n",
      "\n",
      "WITHOUT RAG (model's own knowledge):\n",
      "----------------------------------------\n",
      "The correct answer is 30W-40. This type of motor oil has the right viscosity for the Model T's engine, which was designed to run on lower viscosity oils.\n",
      "\n",
      "The Model T Ford, introduced in 1908, was one of the first automobiles to be mass-produced and widely available to the public. It featured an inline-six engine that required specific types of lubricants to function properly. One crucial aspect of maintaining the Model T's engine was ensuring it ran smoothly with the appropriate motor oil.\n",
      "\n",
      "Motor oil serves several important functions in an internal combustion engine:\n",
      "\n",
      "1. **Lubrication**: Motor oil acts as a barrier between moving parts, reducing friction and wear.\n",
      "2. **Cooling**: Oil helps dissipate heat generated by the engine during operation.\n",
      "3. **Cleaning**: It removes contaminants from the engine components.\n",
      "4. **Sealing**: Some oils have additives that help prevent leaks and seal joints.\n",
      "\n",
      "For the Model T, using the wrong type of motor oil could lead to serious issues such as overheating, increased wear on parts, or even damage to the engine itself. Therefore, selecting the right oil is essential for keeping your vintage car running efficiently.\n",
      "\n",
      "### Why 30W-40?\n",
      "A 30W-40 motor oil is specifically formulated for engines like those found in early cars, including the Model T. Here‚Äôs why it‚Äôs ideal:\n",
      "\n",
      "- **Viscosity Range**: A 30W-40 oil provides a balance between low-temperature fluidity (the ability to flow easily at cold temperatures) and high-temperature stability (resistance to breakdown when heated). This makes it suitable for the Model T's engine, which typically operates under varying conditions but generally requires a moderate viscosity.\n",
      "  \n",
      "- **Compatibility**: These oils are known to perform well with older vehicles due to their compatibility with the materials used in these engines. They do not contain modern additives that might interfere with the original design of the engine.\n",
      "\n",
      "- **Cost-effectiveness**: While higher-quality oils may offer better performance over time, 30W-40 oil is often more affordable and sufficient for most automotive needs without requiring frequent changes.\n",
      "\n",
      "In summary, using 30W-40 motor oil for a Model T Ford ensures that its engine receives the proper lubrication needed to operate efficiently and safely. Always check the manufacturer‚Äôs recommendations for the best oil grade suited to your vehicle‚Äôs specific requirements.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH RAG (using retrieved context):\n",
      "----------------------------------------\n",
      "According to the instructions provided, you should pour in a gallon of new oil when draining the old oil from the crankcase before replacing the time gear, cylinder front cover gasket and cover, and commutator. The text also mentions installing the fan and radiator according to the specified paragraphs. However, there's no mention of using a specific type of oil for the Model T engine. To determine the correct oil, you would need more detailed specifications or recommendations from automotive experts. Based solely on this context, we cannot definitively state what oil to use without additional information.\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 1: Compare WITH vs WITHOUT RAG\n",
    "# ==========================================\n",
    "\n",
    "question = \"What are the specifications for the landing gear?\"  # ‚Üê Use a corpus-specific question!\n",
    "for i, question in enumerate(QUERIES_MODEL_T): # QUERIES_MODEL_T ||| QUERIES_CR\n",
    "    print(f\"\\n\\n{'-'* 70}\\nQuestion {i + 1}: {question}\\n\")\n",
    "    if index.ntotal > 0:\n",
    "        # WITHOUT RAG - just ask the model directly\n",
    "        direct_prompt = f\"\"\"Answer this question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "        \n",
    "        print(\"WITHOUT RAG (model's own knowledge):\")\n",
    "        print(\"-\" * 40)\n",
    "        direct_answer = generate_response(direct_prompt)\n",
    "        print(direct_answer)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "        \n",
    "        # WITH RAG\n",
    "        print(\"WITH RAG (using retrieved context):\")\n",
    "        print(\"-\" * 40)\n",
    "        rag_answer = rag_query(question, top_k=5)\n",
    "        print(rag_answer)\n",
    "    else:\n",
    "        print(\"Please complete the pipeline setup first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 2: Effect of top_k\n",
    "# ==========================================\n",
    "\n",
    "question = \"What safety procedures are required?\"  # ‚Üê Use a corpus-specific question!\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    for k in [1, 3, 5, 10]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TOP_K = {k}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        answer = rag_query(question, top_k=k)\n",
    "        print(answer[:500] + \"...\" if len(answer) > 500 else answer)\n",
    "else:\n",
    "    print(\"Please complete the pipeline setup first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 3: Question the corpus CAN'T answer\n",
    "# ==========================================\n",
    "# Does the model admit it doesn't know, or hallucinate?\n",
    "\n",
    "unanswerable_question = \"What is the CEO's favorite color?\"\n",
    "\n",
    "if index.ntotal > 0:\n",
    "    print(f\"Question: {unanswerable_question}\\n\")\n",
    "    answer = rag_query(unanswerable_question, top_k=5, show_context=True)\n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "else:\n",
    "    print(\"Please complete the pipeline setup first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save/Load Your Index\n",
    "\n",
    "For large corpora, you don't want to re-embed every time. Here's how to persist the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_index(filepath: str):\n",
    "    \"\"\"Save FAISS index and chunks to disk.\"\"\"\n",
    "    faiss.write_index(index, f\"{filepath}.faiss\")\n",
    "    with open(f\"{filepath}.chunks\", 'wb') as f:\n",
    "        pickle.dump(all_chunks, f)\n",
    "    print(f\"‚úì Saved index to {filepath}.faiss\")\n",
    "    print(f\"‚úì Saved chunks to {filepath}.chunks\")\n",
    "\n",
    "def load_saved_index(filepath: str):\n",
    "    \"\"\"Load FAISS index and chunks from disk.\"\"\"\n",
    "    global index, all_chunks\n",
    "    index = faiss.read_index(f\"{filepath}.faiss\")\n",
    "    with open(f\"{filepath}.chunks\", 'rb') as f:\n",
    "        all_chunks = pickle.load(f)\n",
    "    print(f\"‚úì Loaded index with {index.ntotal} vectors\")\n",
    "\n",
    "# Save your index\n",
    "if index.ntotal > 0:\n",
    "    save_index(\"my_rag_index\")\n",
    "else:\n",
    "    print(\"No index to save.\")\n",
    "\n",
    "# Later, to load:\n",
    "# load_saved_index(\"my_rag_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "You've built a complete RAG pipeline from scratch! In the next class, we'll:\n",
    "\n",
    "1. **Improve retrieval** with query rewriting and hybrid search\n",
    "2. **Rebuild with LangChain** to see how frameworks abstract these steps\n",
    "3. **Evaluate systematically** with test questions and metrics\n",
    "\n",
    "### Exercises to try:\n",
    "- Vary chunk size (256, 512, 1024) and measure retrieval quality\n",
    "- Try a different embedding model (`BAAI/bge-small-en-v1.5`)\n",
    "- Try a larger LLM (`Qwen/Qwen2.5-3B-Instruct`) and compare answer quality\n",
    "- Ask questions that require combining information from multiple chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: Device Information\n",
    "\n",
    "Run this cell to see detailed information about your compute environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_device_info():\n",
    "    \"\"\"Print detailed information about available compute devices.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DEVICE INFORMATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nEnvironment: {ENVIRONMENT}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # CUDA\n",
    "    print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # MPS\n",
    "    print(f\"\\nMPS available: {torch.backends.mps.is_available()}\")\n",
    "    print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "    \n",
    "    # Current selection\n",
    "    print(f\"\\n‚Üí Selected device: {DEVICE}\")\n",
    "    print(f\"‚Üí Selected dtype: {DTYPE}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print_device_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csAAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
